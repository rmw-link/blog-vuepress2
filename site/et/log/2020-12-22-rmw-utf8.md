# rmw-utf8 - utf8-kompressioonikodeerimine

Lühike tekstikompressiooni algoritm utf-8 jaoks, mis on optimeeritud hiina keele jaoks ja põhineb rooste programmeerimiskeelel.

Märkus: rmw-utf8 saab tihendada ainult utf-8 teksti, see ei ole üldotstarbeline binaarkompressiooni algoritm.

On olemas [rooste versioon](https://github.com/rmw-link/rmw-utf8) ja [wasm versioon](https://github.com/rmw-lib/rmw-utf8-wasm) javascript'i jaoks.

## Kuidas kasutada

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Kompressioonimäära hindamine

See algoritm on mõeldud lühikeste tekstide tihendamiseks ja tulemused on järgmised. Nagu näete, mida lühem on tekst, seda parem on selle algoritmi pakkimiskiirus.

22467 baidi (umbes 7500 hiina tähemärki) puhul on rmw-utf8 ikka veel parem kui üldised pakkimisalgoritmid.

```
#include compress_test/test.txt
```

Testmasin on MacBook Pro 2015 ( 2,2 GHz Intel Core i7 ).

Testkood on leitav aadressil [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Märkused kasutamise kohta

Kompressioon asendab nii `\r\n` kui ka `\r` tekstis `\n`, mis tähendab, et kokkusurutud ja dekompresseeritud tekst ei pruugi olla identne.

### Ajalugu

`\r``\n` Esimene toob kursori rea algusesse ja teine viib kursori ühe kaadri võrra allapoole.

Kunagi ammu, ammu enne arvuteid, oli olemas masin nimega Teletype Model, mis suutis kirjutada 10 tähemärki sekundis.

Probleem oli selles, et reavahetuse tegemiseks kulus 0,2 sekundit. Kui selle 0,2 sekundi jooksul tuleks uus tegelane, oleks see tegelane kadunud.

Nii tulid arendajad välja mõttega lisada igale reale kaks rea lõpumärki.

Ühte nimetatakse "carriage return", mis annab kirjutusmasinale käsu paigutada trükipea vasakule äärele, ja teist nimetatakse "line feed", mis annab kirjutusmasinale käsu liigutada paber ühe rea võrra allapoole.

Siit pärinevad "line feed" ja "carriage return".

Hiljem, kui leiutati arvuti, rakendati neid kahte mõistet ka arvutite suhtes. Tol ajal oli mälu väga kallis ja mõned teadlased arvasid, et kahe tähemärgi lisamine iga rea lõppu on liiga raiskamine, seega piisab ühest.

Nii et maailm murdus lahti.

Unix/Linuxi süsteemides on iga rea lõpus ainus märk "line feed", `\n`; Windowsi süsteemides on vaikimisi "carriage return + line feed", `\r\n`; Maci süsteemides on vaikimisi "carriage return". " või `\r`.

Kaasaegsed tekstiredaktorid toetavad nüüd `\n` kui sulgemismärki, seega ei ole vaja `\r`.

## Koolitus kohandatud sõnastikud

Pakkimisefekti suurendamiseks on võimalik treenida oma kompressioonisõnastikke erinevate keelte ja tekstiliikide jaoks.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Streaming compression ei ole veel rakendatud

Ühtegi voogedastuse pakkimist ei ole tehtud (minu stsenaarium on ju peamiselt lühikesed tekstid).

Igaüks, kes seda vajab, saab ise veel ühe voogedastuse paketi kokku pakkida.

Näiteks tihendage iga 1 MB ja seejärel registreerige tihendatud sisu baitide arv iga lõigu alguses pärast tihendamist.

## Kodeerimise põhimõtted

Tosinast kuni paarisaja tähemärgini ulatuv tekst, peamiselt hiina keeles, ei sobi üldkasutatavate pakkimisalgoritmide jaoks.

Näiteks testisin ma maailma kõige võimsamat pakkimisalgoritmi [zstd](https://github.com/facebook/zstd) ja see pakkus sageli 42 baiti 62 baidiks (jah, ta suurendas, mitte ei tihendanud), isegi sõnastike treenimisel (ma ei saanud aru, kuidas panna zstd ehitama sõnastikke 3 baidi kaupa; ma kattsin zstd sõnastikku ja see oli täis lühikesi lauseid).

On olemas mõned lühikesele tekstile orienteeritud pakkimisalgoritmid, näiteks [shoco](https://ed-von-schleck.github.io/shoco/) ja [smaz](https://github.com/antirez/smaz), kuid need töötavad ainult inglise keele sarnaste keelte puhul ja võimendavad ikkagi lühikest hiina keelt (nende sõnastikud on vaid mõnesaja tähemärgi pikkused, mis ei ole piisav, nii et isegi sõnastike ümberõpe ei ole teostatav).

Teine pakkimisvõimalus on teksti kodeeringu muutmine.

Kui te teate midagi unicode-kodeerimisest, saate aru, et utf-8 kodeerimisskeem nõuab ühe hiina märgi jaoks kolm baiti salvestusruumi (mis on tegelikult üsna raiskamine).

Gb18030-s võtab üks hiina täht kaks baiti, mis säästab 33% ruumi. Kuid gb18030 ei hõlma kõiki unikoodimärke (see on ainult utf8 alamhulk) ja seda ei saa kasutada.

On olemas standardiseeritud unikood-kompressioonikodeeringud, näiteks [scsu](https://github.com/dop251/scsu)[(mida kasutab SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) ja [utf-c](https://github.com/deNULL/utf-c).

Olen seda [katsetanud](https://denull.github.io/utf-c) ja see on umbes kaks baiti ühe hiina keele kohta, pluss üks lisabait (nt 4 hiina keele puhul on see umbes 2*4+1 = 9 baiti).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Oluline on see, et ma olen otsinud veebist ja ei leidnud nende kahe kodeeringu jaoks ühtegi rooste rakendust.

Nende kodeeringute enda rooste implementatsiooni kirjutamine ei ole võimatu, kuid see nõuab erinevate unicode'i keelte kooditabelite intervallide põhjalikku tundmist, mille õppimine on kallis.

Nii et ma mõtlesin, kas ma võiksin teha üldisemat ja paremat kodeerimis- ja pakkimislahendust.

Unicode'i tähemärkide arv on fikseeritud ja teada ning unicode-13.0.0 skeemil on 143859 tähemärki [(vt siit](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

On täiesti võimalik lugeda iga tähemärgi esinemissagedust ja seejärel seda Hoffmani kodeeringu abil kokku suruda.

Niisiis, kasutades mõningaid hiina korpuseid, hakkasin lugema sõnade sagedusi.

Korpus on järgmine.

* [Vikipeedia hiina korpus](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown veebiromaani roomik](https://github.com/ma6254/FictionDown) (versioon, mis on välja antud, roomib kehtetuid peatükke ikka ja jälle, seega on vaja peaversiooni `go get github.com/ma6254/FictionDown@master`).
* [Weibo roomik](https://github.com/gcxfd/weibo-crawler)
* [DHT roomik BT võrgu jaoks](https://github.com/gcxfd/bt-spider)
* [Mõned roomikud kirjutatud off the cuff, vt koodi ämblik kataloogi](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Tulemused on head, kolm hiina märki saab kokku suruda 5 baidini, mis on juba üle gb18030 kompressiooni.

Lisaks mõtlesin, kas ma võiksin Hoffmani sõnaraamatusse lisada tavalisi sõnu, et veelgi optimeerida kompressiooni efekti.

Nii et ma tegin sõnaraamatu sagedamini kasutatavate sõnadega (tihendatud üle 500 KB), kasutades [treeningu algoritmi treeningu kataloogis](https://github.com/rmw-link/rmw-utf8/tree/master/train) sõnade eraldamiseks + ngramm.

Proovisin seda ja see purustab kõik turul olevad kompressioonialgoritmid.

Lahe.