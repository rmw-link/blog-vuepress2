# rmw-utf8 - kodowanie kompresji utf8

Algorytm kompresji krótkich tekstów dla utf-8, zoptymalizowany dla języka chińskiego, oparty na języku programowania rust.

Uwaga: rmw-utf8 może kompresować tylko tekst utf-8, nie jest to algorytm kompresji binarnej ogólnego przeznaczenia.

Istnieje [wersja rust](https://github.com/rmw-link/rmw-utf8) i [wersja wasm](https://github.com/rmw-lib/rmw-utf8-wasm) dla języka javascript.

## Jak używać

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Ocena stopnia kompresji

Algorytm ten jest przeznaczony do kompresji krótkich tekstów, a jego wyniki są następujące. Jak widać, im krótszy jest tekst, tym lepszy jest stopień kompresji tego algorytmu.

Przy 22467 bajtach (około 7500 znaków chińskich) rmw-utf8 nadal przewyższa ogólne algorytmy kompresji.

```
#include compress_test/test.txt
```

Komputer testowy to MacBook Pro 2015 ( 2,2 GHz Intel Core i7 ).

Kod testu można znaleźć pod adresem [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Uwagi dotyczące użytkowania

Kompresja zastępuje zarówno `\r\n`, jak i `\r` w tekście stroną `\n`, co oznacza, że tekst skompresowany i zdekompresowany mogą nie być identyczne.

### Historia

`\r``\n` Pierwsza z nich powoduje przesunięcie kursora na początek wiersza, a druga przesuwa go o jedną klatkę w dół.

Dawno, dawno temu, na długo przed komputerami, istniała maszyna zwana Teletype Model, która potrafiła pisać 10 znaków na sekundę.

Problem polegał na tym, że zrobienie przerwy w linii zajmowało 0,2 sekundy. Gdyby w ciągu tych 0,2 sekundy pojawiła się nowa postać, postać ta zostałaby utracona.

Dlatego programiści wpadli na pomysł, aby do każdego wiersza dodać dwa znaki końca linii.

Jeden z nich to "carriage return", który nakazuje maszynie do pisania ustawić głowicę drukującą przy lewej krawędzi, a drugi to "line feed", który nakazuje maszynie do pisania przesunąć papier o jedną linię w dół.

Stąd właśnie pochodzą pojęcia "line feed" i "carriage return".

Później, gdy wynaleziono komputer, te dwie koncepcje zastosowano również do komputerów. W tamtych czasach pamięć była bardzo droga i niektórzy naukowcy uważali, że dodawanie dwóch znaków na końcu każdego wiersza jest zbyt marnotrawne, więc wystarczyłby jeden.

Tak oto świat stanął otworem.

W systemach Unix/Linux jedynym znakiem na końcu każdego wiersza jest "line feed", `\n`; w systemach Windows domyślnie jest to "carriage return + line feed", `\r\n`; w systemach Mac domyślnie jest to "carriage return " lub `\r`.

Nowoczesne edytory tekstu obsługują obecnie `\n` jako znak zamykający, więc nie ma potrzeby stosowania `\r`.

## Szkolenie słowników niestandardowych

Możliwe jest wytrenowanie własnego zestawu słowników kompresji dla różnych języków i typów tekstu w celu zwiększenia efektu kompresji.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Nie zaimplementowano jeszcze kompresji strumieniowej

Nie zastosowano kompresji strumieniowej (w końcu mój scenariusz obejmuje głównie krótkie teksty).

Każdy, kto tego potrzebuje, może samodzielnie przeprowadzić kolejną kompresję strumieniową.

Na przykład kompresuj co 1 MB, a następnie zapisuj liczbę bajtów skompresowanej zawartości na początku każdego akapitu po kompresji.

## Zasady kodowania

Tekst liczący od kilkunastu do kilkuset znaków, głównie w języku chińskim, nie nadaje się do stosowania algorytmów kompresji ogólnego przeznaczenia.

Na przykład testowałem [zstd](https://github.com/facebook/zstd), najpotężniejszy algorytm kompresji na świecie, który często kompresował 42 bajty na 62 bajty (tak, powiększał zamiast kompresować), nawet podczas szkolenia słowników (nie udało mi się wymyślić, jak sprawić, by zstd tworzył słowniki co 3 bajty; sprawdziłem słownik zstd i był on pełen krótkich zdań).

Istnieją pewne algorytmy kompresji krótkich tekstów, takie jak [shoco](https://ed-von-schleck.github.io/shoco/) i [smaz](https://github.com/antirez/smaz), ale działają one tylko dla języków zbliżonych do angielskiego i nadal wzmacniają krótki język chiński (ich słowniki mają tylko kilkaset znaków, co jest niewystarczające, więc nawet ponowne przeszkolenie słowników nie jest wykonalne).

Inną opcją kompresji jest zmiana kodowania tekstu.

Jeśli wiesz cokolwiek na temat kodowania unicode, zrozumiesz, że schemat kodowania utf-8 wymaga trzech bajtów przestrzeni dyskowej na jeden znak chiński (co w rzeczywistości jest dość marnotrawne).

W gb18030 jeden chiński znak zajmuje dwa bajty, co pozwala zaoszczędzić 33% miejsca. Jednak gb18030 nie obejmuje wszystkich znaków unicode (jest tylko podzbiorem utf8) i nie można go używać.

Istnieją znormalizowane kodowania kompresji unicode, takie jak [scsu](https://github.com/dop251/scsu)[(używane przez SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) i [utf-c](https://github.com/deNULL/utf-c).

[Sprawdziłem](https://denull.github.io/utf-c) to i na każdy język chiński przypadają dwa bajty plus dodatkowy bajt (np. 4 języki chińskie to około 2*4+1 = 9 bajtów).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Najważniejsze jest to, że przeszukałem sieć i nie znalazłem żadnych implementacji rust dla tych dwóch kodowań.

Napisanie własnej implementacji tych kodowań w języku rust nie jest niemożliwe, ale wymaga dogłębnego zrozumienia interwałów tablic kodowych różnych języków unicode, co jest kosztowne.

Zastanawiałem się więc, czy można stworzyć bardziej ogólne i lepsze rozwiązanie do kodowania i kompresji.

Liczba znaków w standardzie unicode jest stała i znana, a schemat unicode-13.0.0 zawiera 143859 znaków [(zob. tutaj](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Doskonale można policzyć częstotliwość występowania każdego znaku, a następnie skompresować go przy użyciu kodowania Hoffmana.

Wykorzystując pewien korpus chiński, zacząłem liczyć częstotliwość występowania słów.

Korpus jest następujący.

* [Korpus chiński w Wikipedii](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel crawler](https://github.com/ma6254/FictionDown) (wersja release będzie w kółko indeksować nieważne rozdziały, dlatego potrzebna jest wersja master `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler dla sieci BT](https://github.com/gcxfd/bt-spider)
* [Kilka crawlerów napisanych od niechcenia, patrz katalog code spider](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Wyniki są dobre, trzy chińskie znaki można skompresować do 5 bajtów, co już przekracza możliwości kompresji gb18030.

Zastanawiałem się też, czy można dodać do słownika Hoffmana popularne słowa, aby jeszcze bardziej zoptymalizować efekt kompresji.

Stworzyłem więc słownik zawierający często używane słowa (skompresowany do ponad 500 KB), używając [algorytmu treningowego w katalogu train](https://github.com/rmw-link/rmw-utf8/tree/master/train) do separacji słów + ngram.

Wypróbowałem go i miażdży każdy algorytm kompresji dostępny na rynku.

Fajnie.