# rmw-utf8 - utf8 tömörítési kódolás

Rövid szövegtömörítési algoritmus az utf-8-hoz, kínai nyelvre optimalizálva, a rust programozási nyelven alapulva.

Megjegyzés: az rmw-utf8 csak utf-8 szöveg tömörítésére alkalmas, nem általános célú bináris tömörítő algoritmus.

Létezik egy [rust](https://github.com/rmw-link/rmw-utf8) és egy [wasm](https://github.com/rmw-lib/rmw-utf8-wasm) verzió a javascripthez.

## Hogyan kell használni

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## A tömörítési arány értékelése

Ezt az algoritmust rövid szövegek tömörítésére tervezték, és az eredmények a következők. Mint látható, minél rövidebb a szöveg, annál jobb a tömörítési arány az algoritmusban.

22467 bájt (kb. 7500 kínai karakter) esetén az rmw-utf8 még mindig felülmúlja az általános tömörítő algoritmusokat.

```
#include compress_test/test.txt
```

A tesztgép egy MacBook Pro 2015 ( 2,2 GHz Intel Core i7 )

A tesztkód megtalálható a [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test) oldalon.

## Használati megjegyzések

A tömörítés a szövegben a `\r\n` és a `\r` címet is `\n`címmel helyettesíti, ami azt jelenti, hogy a tömörített és a dekomprimált szöveg nem feltétlenül azonos.

### Történelem

`\r``\n` Az előbbi a kurzort a sor elejére viszi, az utóbbi pedig egy képkockával lejjebb mozgatja a kurzort.

Egyszer volt, hol nem volt, jóval a számítógépek előtt, volt egy gép, a Teletype Model, amely másodpercenként 10 karaktert tudott gépelni.

A probléma az volt, hogy 0,2 másodpercig tartott egy sortörés. Ha ebben a 0,2 másodpercben egy új karakter jönne át, a karakter elveszne.

Ezért a fejlesztők azzal az ötlettel álltak elő, hogy minden sorhoz két sorvég karaktert adnak hozzá.

Az egyik a "kocsis visszatérés", amely az írógépet arra utasítja, hogy a nyomtatófejet a bal szélére állítsa, a másik pedig a "sortovábbítás", amely az írógépet arra utasítja, hogy a papírt egy sorral lejjebb vigye.

Innen származik a "line feed" és a "carriage return".

Később, amikor feltalálták a számítógépet, ezt a két fogalmat a számítógépekre is alkalmazták. Abban az időben a memória nagyon drága volt, és néhány tudós úgy gondolta, hogy túl pazarló lenne két karaktert hozzáadni minden sor végére, ezért elég volt egy is.

Így a világ megrepedt.

Unix/Linux rendszereken az egyetlen karakter minden sor végén a "line feed", `\n`; Windows rendszereken az alapértelmezett karakter a "carriage return + line feed", `\r\n`; Mac rendszereken az alapértelmezett karakter a "carriage return". " vagy `\r`.

A modern szövegszerkesztők már támogatják a `\n` címet mint záró karaktert, így nincs szükség a `\r` címre.

## Egyéni szótárak képzése

A tömörítési hatás fokozása érdekében lehetőség van saját tömörítési szótárkészletet képezni a különböző nyelvekhez és szövegtípusokhoz.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## A streaming tömörítés még nem valósult meg

Nem történt streaming tömörítés (végül is a forgatókönyvem főleg rövid szövegekből áll).

Bárki, akinek szüksége van rá, saját maga is csomagolhat egy másik streaming tömörítést.

Például tömörítsen 1 MB-onként, majd a tömörítés után minden egyes bekezdés elején rögzítse a tömörített tartalom bájtjainak számát.

## Kódolási elvek

A tucatnyi és néhány száz karakter közötti, főként kínai nyelvű szövegek nem alkalmasak az általános célú tömörítő algoritmusok számára.

Például teszteltem a [zstd-t](https://github.com/facebook/zstd), a világ legerősebb tömörítő algoritmusát, és gyakran 42 bájtot tömörített 62 bájtba (igen, nagyított a tömörítés helyett), még szótárak képzésekor is (nem tudtam rájönni, hogyan lehet rávenni a zstd-t, hogy 3 bájtos lépésekben építsen szótárakat; megnéztem a zstd szótárát, és tele volt rövid mondatokkal).

Létezik néhány rövid szövegre orientált tömörítő algoritmus, mint például a [shoco](https://ed-von-schleck.github.io/shoco/) és a [smaz](https://github.com/antirez/smaz), de ezek csak az angolhoz hasonló nyelvekre működnek, és még mindig felerősítik a rövid kínai nyelvet (szótáraik csak néhány száz karakter hosszúak, ami nem elég, így még a szótárak átképzése sem kivitelezhető).

Egy másik tömörítési lehetőség a szöveg kódolásának megváltoztatása.

Ha bármit tudsz az unicode kódolásról, akkor megérted, hogy az utf-8 kódolási séma három bájtnyi tárhelyet igényel egy kínai karakterhez (ami valójában elég pazarló).

A gb18030-ban egy kínai karakter két bájtot foglal el, így a hely 33%-át takarítja meg. A gb18030 azonban nem fedi le az összes unicode karaktert (ez csak az utf8 egy részhalmaza), ezért nem használható.

Vannak szabványosított unicode tömörítési kódolások, mint például az [scsu](https://github.com/dop251/scsu)[(amelyet az SqlServer használ](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) és [az utf-c](https://github.com/deNULL/utf-c).

Ezt [teszteltem](https://denull.github.io/utf-c), és ez körülbelül két bájt kínai nyelvenként, plusz egy plusz bájt (pl. 4 kínai nyelven ez körülbelül 2*4+1 = 9 bájt).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

A legfontosabb dolog az, hogy átkutattam az internetet, és nem találtam rozsda implementációkat erre a két kódolásra.

Ezeknek a kódolásoknak a saját rosta implementációját megírni nem lehetetlen, de ehhez a különböző unicode nyelvek kódtábla-intervallumainak mélyreható ismerete szükséges, amit drága megtanulni.

Ezért arra gondoltam, hogy tudnék-e egy általánosabb és jobb kódolási és tömörítési megoldást készíteni.

Az unicode karakterek száma fix és ismert, az unicode-13.0.0 séma 143859 karaktert tartalmaz [(lásd itt](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Tökéletesen lehetséges megszámolni az egyes karakterek előfordulási gyakoriságát, majd Hoffman-kódolással tömöríteni.

Ezért néhány kínai korpuszt felhasználva elkezdtem számolni a szavak gyakoriságát.

A korpusz a következő.

* [Wikipedia kínai korpusz](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown webes regénykúszó](https://github.com/ma6254/FictionDown) (a kiadásra szánt verzió újra és újra átnézi az érvénytelen fejezeteket, ezért a master verzióra van szükség `go get github.com/ma6254/FictionDown@master`)
* [Weibo kúszó](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler a BT hálózathoz](https://github.com/gcxfd/bt-spider)
* [Néhány kúszóprogramok írt off the cuff, lásd a kód pók könyvtárban](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Az eredmények jók, három kínai karaktert 5 bájtra lehet tömöríteni, ami már meghaladja a gb18030 tömörítését.

Azon gondolkodtam továbbá, hogy a tömörítési hatás további optimalizálása érdekében hozzáadhatnám-e a Hoffman szótárához a gyakori szavakat.

Ezért készítettem egy szótárat a gyakran használt szavakkal (több mint 500 KB-ra tömörítve) a [train könyvtárban található képzési algoritmus](https://github.com/rmw-link/rmw-utf8/tree/master/train) segítségével a szavak szétválasztásához + ngramm.

Kipróbáltam, és a piacon lévő összes tömörítési algoritmust legyőzi.

Király.