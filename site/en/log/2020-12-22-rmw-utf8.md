# rmw-utf8 - utf8 compression encoding

Short text compression algorithm for utf-8, optimized for Chinese, based on the rust programming language.

Note: rmw-utf8 can only compress utf-8 text, it is not a general-purpose binary compression algorithm.

There is a [rust version](https://github.com/rmw-link/rmw-utf8) and a [wasm version](https://github.com/rmw-lib/rmw-utf8-wasm) for javascript.

## How to use

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Compression rate evaluation

This algorithm is for short text compression, and the results of the evaluation are as follows. As you can see, the shorter the text, the better the compression rate of this algorithm.

At 22467 bytes (about 7500 Chinese characters), the compression efficiency of rmw-utf8 is still better than the general compression algorithm.

```
#include compress_test/test.txt
```

The test machine is a MacBook Pro 2015 ( 2.2 GHz Intel Core i7 )

The test code can be found in [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Usage Notes

Compression replaces both `\r\n` and `\r` in the text with `\n`, which means that the compressed and decompressed text may not be identical.

### History

`\r``\n` The former brings the cursor to the beginning of the line, while the latter moves the cursor down one frame.

A long time ago, before computers were born, there was a machine called a Teletype Model that could type 10 characters per second.

It had a problem that it took 0.2 seconds to make a line break. If in this 0.2 seconds, there is a new character passed, then the character will be lost.

So, the developers came up with a way to add two end characters after each line to indicate the end.

One is called "carriage return", which tells the typewriter to position the print head at the left border; the other is called "line feed", which tells the typewriter to move the paper down one line.

This is the origin of "line feed" and "carriage return".

Later, the computer was invented, these two concepts were also general to the computer. At that time, memory was very expensive, and some scientists thought it was too wasteful to add two characters at the end of each line, so adding one would do.

So, the world cracked open.

For Unix/Linux systems, the only character at the end of each line is "line feed", which is `\n`; for Windows systems, the default is "carriage return + line feed", which is `\r\n`; for Mac systems, the default is "carriage return " which is `\r`.

Modern text editors now support `\n` as a closing character, so there is no need for `\r` to exist.

## Training custom dictionaries

For different languages and types of text, you can train your own set of compression dictionaries to enhance the compression effect.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Streaming compression not yet implemented

No streaming compression has been done (after all, my scenario is mainly short texts).

Those who need it can package another streaming compression by themselves.

For example, every 1MB compression, after the compression of the beginning of each paragraph and then record the number of bytes of the compressed content.

## Encoding principle

A dozen to a couple of hundred words, mainly Chinese text, universal compression algorithm is not suitable.

For example, I tested [zstd](https://github.com/facebook/zstd), the most powerful compression algorithm in the world, and it often compressed 42 bytes into 62 bytes (yes, it enlarged instead of compressing), even after training a dictionary (I couldn't figure out how to make zstd create a dictionary in 3-byte units, I catted zstd's dictionary and it was full of short sentences).

There are some compression algorithms for short texts, such as [shoco](https://ed-von-schleck.github.io/shoco/) and [smaz](https://github.com/antirez/smaz), but they only work for English-like languages and still amplify short Chinese (their dictionaries have only a few hundred characters, which is not enough, so even retraining the dictionaries is not feasible).

Another compression option is to change the encoding of the text.

If you know about unicode encoding, you will naturally understand that a Chinese character in utf-8 encoding scheme needs three bytes of storage space (which is actually quite wasteful).

In gb18030, one Chinese character is two bytes, which saves 33% of space. However, gb18030 does not cover all unicode characters (it is only a subset of utf8), so it is not possible to use it.

There are standardized unicode compression encodings, such as [scsu](https://github.com/dop251/scsu) ( [which is used by SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) and [utf-c](https://github.com/deNULL/utf-c).

I [tested](https://denull.github.io/utf-c) it, and it's about two bytes per Chinese, plus one extra byte (for example, 4 Chinese is about 2*4+1 = 9 bytes).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

The key thing is that I searched the whole web and couldn't find any rust implementations for these two encodings.

It's not impossible to write your own rust implementation of these encodings, but it requires a deep understanding of the code table interval of various languages in unicode, which is costly to learn, lying flat, not wanting to learn, and not being able to manage.

So I thought, can we make a more general and better encoding compression scheme.

The number of unicode characters is fixed and known, and the unicode-13.0.0 scheme has 143859 characters [(see here](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

It is perfectly possible to count the occurrence frequency of each character and then compress it with Hoffman encoding.

So, some Chinese corpus was used to start counting the word frequencies.

The corpus is as follows.

* [Wikipedia Chinese corpus](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel c](https://github.com/ma6254/FictionDown) rawler (the release version will repeatedly crawl the failed chapters, so you need to use the master version `go get github.com/ma6254/FictionDown@master`)
* [Microblog crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler for the BT network](https://github.com/gcxfd/bt-spider)
* [A few crawlers written on the fly, see code spider directory](https://github.com/rmw-link/utf8_compress/tree/master/spider)

The result is good, three Chinese can be compressed to 5 bytes, which is beyond the compression effect of gb18030.

I further wondered if I could also add common words to Hoffman's dictionary to further optimize the compression effect.

So, I made a dictionary with commonly used words (more than 500 KB after compression) using [the training algorithm in the train directory](https://github.com/rmw-link/rmw-utf8/tree/master/train) with word separation + ngram.

Tried it out and it crushed all compression algorithms on the market.

Cool.