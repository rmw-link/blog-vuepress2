# rmw-utf8 - kodning af utf8-komprimering

En kort tekstkomprimeringsalgoritme for utf-8, optimeret til kinesisk, baseret på programmeringssproget rust.

Bemærk: rmw-utf8 kan kun komprimere utf-8 tekst, det er ikke en binær komprimeringsalgoritme til generelle formål.

Der findes en [rust-version](https://github.com/rmw-link/rmw-utf8) og en [wasm-version](https://github.com/rmw-lib/rmw-utf8-wasm) til javascript.

## Sådan bruger du

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Evaluering af kompressionshastighed

Denne algoritme er designet til at komprimere korte tekster, og resultaterne er som følger. Som du kan se, er denne algoritme bedre komprimeret, jo kortere teksten er, jo bedre er komprimeringsraten.

Med 22467 bytes (ca. 7500 kinesiske tegn) klarer rmw-utf8 sig stadig bedre end generiske komprimeringsalgoritmer.

```
#include compress_test/test.txt
```

Testmaskinen er en MacBook Pro 2015 ( 2,2 GHz Intel Core i7 )

Testkoden kan findes på [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Bemærkninger om anvendelse

Ved komprimering erstattes både `\r\n` og `\r` i teksten med `\n`, hvilket betyder, at den komprimerede og den dekomprimerede tekst ikke nødvendigvis er identiske.

### Historie

`\r``\n` Førstnævnte bringer markøren til linjens begyndelse, og sidstnævnte flytter markøren et billede nedad.

Der var engang, længe før computerne, en maskine, der hed en teletype-model, som kunne skrive 10 tegn i sekundet.

Problemet var, at det tog 0,2 sekunder at lave et linjeskift. Hvis der kom en ny karakter i løbet af disse 0,2 sekunder, ville karakteren være tabt.

Derfor fandt udviklerne på at tilføje to tegn til slutningen af hver linje.

Den ene kaldes en "vognretur", som fortæller skrivemaskinen, at den skal placere skrivehovedet ved venstre kant, og den anden kaldes en "linjeføring", som fortæller skrivemaskinen, at den skal flytte papiret en linje nedad.

Det er herfra, at "line feed" og "carriage return" kommer.

Senere, da computeren blev opfundet, blev disse to begreber også anvendt på computere. På den tid var hukommelse meget dyrt, og nogle forskere mente, at det var for spildt at tilføje to tegn i slutningen af hver linje, så et enkelt var nok.

Så verden brød op.

På Unix/Linux-systemer er det eneste tegn i slutningen af hver linje et "linjetræk", `\n`; på Windows-systemer er standardværdien "vogn retur + linjetræk", `\r\n`; på Mac-systemer er standardværdien "vogn retur". " eller `\r`.

Moderne tekstredigeringsprogrammer understøtter nu `\n` som afsluttende tegn, så det er ikke nødvendigt med `\r`.

## Træning af brugerdefinerede ordbøger

Det er muligt at træne dit eget sæt kompressionsordbøger for forskellige sprog og teksttyper for at forbedre komprimeringseffekten.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Streaming-komprimering er endnu ikke implementeret

Der er ikke foretaget nogen streamingkomprimering (mit scenarie er trods alt hovedsageligt korte tekster).

Alle, der har brug for det, kan selv pakke en anden streamingkomprimering.

Komprimer f.eks. hver 1 MB, og registrer derefter antallet af bytes af komprimeret indhold i begyndelsen af hvert afsnit efter komprimering.

## Principper for kodning

Tekst med et dusin til et par hundrede tegn, hovedsagelig på kinesisk, er ikke egnet til generelle komprimeringsalgoritmer.

Jeg testede f.eks. [zstd](https://github.com/facebook/zstd), verdens mest kraftfulde komprimeringsalgoritme, og den komprimerede ofte 42 bytes til 62 bytes (ja, den forstørrede i stedet for at komprimere), selv når den trænede ordbøger (jeg kunne ikke finde ud af at få zstd til at opbygge ordbøger i 3-byte intervaller; jeg gennemgik zstds ordbog, og den var fuld af korte sætninger).

Der findes nogle korte tekstorienterede komprimeringsalgoritmer som [shoco](https://ed-von-schleck.github.io/shoco/) og [smaz](https://github.com/antirez/smaz), men de fungerer kun for engelsklignende sprog og forstærker stadig kort kinesisk (deres ordbøger er kun nogle få hundrede tegn lange, hvilket ikke er nok, så det er ikke engang muligt at omskole ordbøgerne).

En anden komprimeringsmulighed er at ændre kodningen af teksten.

Hvis du ved noget om unicode-kodning, vil du forstå, at utf-8-kodningssystemet kræver tre bytes lagerplads for ét kinesisk tegn (hvilket faktisk er ret spildt).

I gb18030 fylder et kinesisk tegn to bytes, hvilket sparer 33 % af pladsen. gb18030 dækker dog ikke alle Unicode-tegn (det er kun en delmængde af utf8) og kan ikke bruges.

Der findes standardiserede unicode-komprimeringskodninger såsom [scsu](https://github.com/dop251/scsu)[(som bruges af SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) og [utf-c](https://github.com/deNULL/utf-c).

Jeg har [testet](https://denull.github.io/utf-c) dette, og det er ca. to bytes pr. kineser plus en ekstra byte (f.eks. er 4 kinesere ca. 2*4+1 = 9 bytes).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Det vigtigste er, at jeg har søgt på nettet og ikke fundet nogen rust-implementeringer for disse to kodninger.

Det er ikke umuligt at skrive min egen rust-implementering af disse kodninger, men det kræver en dyb forståelse af kodetabelintervallerne i forskellige unicode-sprog, hvilket er dyrt at lære.

Så jeg tænkte på, om jeg kunne lave en mere generel og bedre kodnings- og komprimeringsløsning.

Antallet af tegn i unicode er fast og kendt, og unicode-13.0.0.0-ordningen har 143859 tegn ( [se her](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Det er fuldt ud muligt at tælle hyppigheden af hvert enkelt tegn og derefter komprimere det ved hjælp af Hoffman-kodning.

Så jeg begyndte at tælle ordfrekvenserne ved hjælp af et kinesisk korpus.

Korpusset er som følger.

* [Wikipedia kinesisk korpus](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel crawler](https://github.com/ma6254/FictionDown) (udgivelsesversionen vil gentagne gange gennemgå ugyldige kapitler igen og igen, så masterversionen er nødvendig `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT-crawler til BT-netværket](https://github.com/gcxfd/bt-spider)
* [Et par crawlere skrevet uden videre, se kode spider mappe](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Resultaterne er gode, tre kinesiske tegn kan komprimeres til 5 bytes, hvilket allerede er mere end gb18030's komprimering.

Jeg tænkte desuden på, om jeg kunne tilføje almindelige ord til Hoffmans ordbog for yderligere at optimere kompressionseffekten.

Så jeg lavede en ordbog med almindeligt anvendte ord (komprimeret til over 500 KB) ved hjælp af [træningsalgoritmen i train-mappen](https://github.com/rmw-link/rmw-utf8/tree/master/train) til ordseparation + ngram.

Jeg har prøvet det, og det slår alle kompressionsalgoritmer på markedet.

Cool.