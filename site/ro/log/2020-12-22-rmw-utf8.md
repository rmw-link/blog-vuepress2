# rmw-utf8 - codificarea compresiei utf8

Un algoritm de compresie a textului scurt pentru utf-8, optimizat pentru limba chineză, bazat pe limbajul de programare rust.

Notă: rmw-utf8 poate comprima doar text în format utf-8, nu este un algoritm de compresie binară de uz general.

Există o [versiune rust](https://github.com/rmw-link/rmw-utf8) și o [versiune wasm](https://github.com/rmw-lib/rmw-utf8-wasm) pentru javascript.

## Cum se utilizează

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Evaluarea ratei de compresie

Acest algoritm este conceput pentru a comprima texte scurte, iar rezultatele sunt următoarele. După cum puteți vedea, cu cât textul este mai scurt, cu atât mai bună este rata de compresie a acestui algoritm.

La 22467 octeți (aproximativ 7500 de caractere chinezești), rmw-utf8 încă depășește algoritmii de compresie generici.

```
#include compress_test/test.txt
```

Mașina de test este un MacBook Pro 2015 ( 2.2 GHz Intel Core i7 )

Codul de test poate fi găsit la [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Note privind utilizarea

Compresia înlocuiește în text atât `\r\n`, cât și `\r` cu `\n`, ceea ce înseamnă că textul comprimat și cel decomprimat pot să nu fie identice.

### Istorie

`\r``\n` Prima aduce cursorul la începutul liniei, iar cea de-a doua mută cursorul în jos cu un cadru.

A fost odată, cu mult înainte de apariția computerelor, o mașină numită Teletype Model care putea tasta 10 caractere pe secundă.

Problema era că era nevoie de 0,2 secunde pentru a face o întrerupere de linie. Dacă un nou personaj ar fi apărut în acele 0,2 secunde, personajul ar fi fost pierdut.

Astfel, dezvoltatorii au venit cu ideea de a adăuga două caractere de sfârșit de linie la fiecare linie.

Unul se numește "revenire la cărucior", care îi spune mașinii de scris să poziționeze capul de imprimare la marginea stângă, iar celălalt se numește "avans de linie", care îi spune mașinii de scris să deplaseze hârtia pe o linie.

De aici provin "line feed" și "carriage return".

Mai târziu, când a fost inventat calculatorul, aceste două concepte au fost aplicate și la calculatoare. La acea vreme, memoria era foarte scumpă, iar unii oameni de știință au considerat că era prea costisitor să adauge două caractere la sfârșitul fiecărui rând, așa că era suficient unul singur.

Așa că lumea s-a deschis.

Pe sistemele Unix/Linux, singurul caracter de la sfârșitul fiecărei linii este "line feed", `\n`; pe sistemele Windows, caracterul implicit este "carriage return + line feed", `\r\n`; pe sistemele Mac, caracterul implicit este "carriage return". " sau `\r`.

Editorii de text moderni acceptă acum `\n` ca caracter de închidere, astfel încât nu mai este nevoie de `\r`.

## Formarea dicționarelor personalizate

Este posibil să vă pregătiți propriul set de dicționare de compresie pentru diferite limbi și tipuri de text pentru a îmbunătăți efectul de compresie.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Compresia fluxurilor nu este încă implementată

Nu a fost efectuată nicio compresie de streaming (la urma urmei, scenariul meu este format în principal din texte scurte).

Oricine are nevoie de ea poate împacheta singur o altă compresie de streaming.

De exemplu, comprimați fiecare 1 MB și apoi înregistrați numărul de octeți de conținut comprimat la începutul fiecărui paragraf după compresie.

## Principii de codificare

Textele de o duzină până la câteva sute de caractere, în principal în limba chineză, nu sunt potrivite pentru algoritmi de compresie de uz general.

De exemplu, am testat [zstd](https://github.com/facebook/zstd), cel mai puternic algoritm de compresie din lume, și deseori a comprimat 42 de octeți în 62 de octeți (da, a mărit în loc să comprime), chiar și atunci când a antrenat dicționare (nu am reușit să-mi dau seama cum să fac ca zstd să construiască dicționare în trepte de 3 octeți; am verificat dicționarul lui zstd și era plin de propoziții scurte).

Există câțiva algoritmi de compresie orientați spre text scurt, cum ar fi [shoco](https://ed-von-schleck.github.io/shoco/) și [smaz](https://github.com/antirez/smaz), dar aceștia funcționează doar pentru limbi asemănătoare limbii engleze și încă amplifică chineza scurtă (dicționarele lor au doar câteva sute de caractere, ceea ce nu este suficient, astfel încât nici măcar recalificarea dicționarelor nu este fezabilă).

O altă opțiune de compresie este schimbarea codificării textului.

Dacă știți ceva despre codificarea Unicode, veți înțelege că schema de codificare utf-8 necesită trei octeți de spațiu de stocare pentru un caracter chinezesc (ceea ce este, de fapt, destul de inutil).

În gb18030, un caracter chinezesc ocupă doi octeți, economisind 33% din spațiu. Cu toate acestea, gb18030 nu acoperă toate caracterele unicode (este doar un subset al utf8) și nu poate fi utilizat.

Există codificări standardizate de compresie unicode, cum ar fi [scsu](https://github.com/dop251/scsu)[(care este utilizat de SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) și [utf-c](https://github.com/deNULL/utf-c).

Am [testat](https://denull.github.io/utf-c) acest lucru și este vorba de aproximativ doi octeți pentru fiecare chineză, plus un octet suplimentar (de exemplu, 4 chineze reprezintă aproximativ 2*4+1 = 9 octeți).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Principalul lucru este că am căutat pe internet și nu am găsit nicio implementare Rust pentru aceste două codificări.

Scrierea propriei mele implementări Rust a acestor codificări nu este imposibilă, dar necesită o înțelegere profundă a intervalelor de tabele de cod ale diferitelor limbaje Unicode, care este costisitoare de învățat.

Așa că m-am întrebat dacă aș putea face o soluție mai generală și mai bună de codificare și compresie.

Numărul de caractere în unicode este fix și cunoscut, iar schema unicode-13.0.0.0 are 143859 caractere [(a se vedea aici](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Este perfect posibil să numărați frecvența de apariție a fiecărui caracter și apoi să îl comprimați folosind codificarea Hoffman.

Așadar, folosind un corpus chinezesc, am început să număr frecvențele cuvintelor.

Corpusul este următorul.

* [Corpusul chinezesc Wikipedia](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel crawler](https://github.com/ma6254/FictionDown) (versiunea de lansare va răsfoi în mod repetat capitole invalide, din nou și din nou, așa că este nevoie de versiunea master `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler pentru rețeaua BT](https://github.com/gcxfd/bt-spider)
* [Câteva crawlere scrise din mers, vedeți directorul de coduri spider](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Rezultatele sunt bune, trei caractere chinezești pot fi comprimate la 5 octeți, ceea ce este deja peste compresia gb18030.

M-am mai întrebat dacă aș putea adăuga cuvinte comune în dicționarul lui Hoffman pentru a optimiza și mai mult efectul de compresie.

Așa că am făcut un dicționar cu cuvinte utilizate în mod obișnuit (comprimat la peste 500 KB) folosind [algoritmul de instruire din directorul train](https://github.com/rmw-link/rmw-utf8/tree/master/train) pentru separarea cuvintelor + ngramă.

L-am testat și zdrobește orice algoritm de compresie de pe piață.

Ești în regulă.