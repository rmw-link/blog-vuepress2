# rmw-utf8 - utf8 compressie codering

Een kort tekstcompressie-algoritme voor utf-8, geoptimaliseerd voor Chinees, gebaseerd op de programmeertaal Rust.

Opmerking: rmw-utf8 kan alleen utf-8 tekst comprimeren, het is geen binaire compressie-algoritme voor algemene doeleinden.

Er is een [roest versie](https://github.com/rmw-link/rmw-utf8) en een [wasm versie](https://github.com/rmw-lib/rmw-utf8-wasm) voor javascript.

## Hoe te gebruiken

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Evaluatie van de compressiesnelheid

Dit algoritme is ontworpen om korte teksten te comprimeren en de resultaten zijn als volgt. Zoals u kunt zien, hoe korter de tekst, hoe beter de compressie van dit algoritme.

Met 22467 bytes (ongeveer 7500 Chinese karakters) presteert rmw-utf8 nog steeds beter dan generieke compressiealgoritmen.

```
#include compress_test/test.txt
```

De testmachine is een MacBook Pro 2015 ( 2.2 GHz Intel Core i7 )

De testcode kan worden gevonden op [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Opmerkingen over het gebruik

Compressie vervangt zowel `\r\n` als `\r` in de tekst door `\n`, wat betekent dat de gecomprimeerde en gedecomprimeerde tekst mogelijk niet identiek zijn.

### Geschiedenis

`\r``\n` De eerste brengt de cursor naar het begin van de regel en de tweede verplaatst de cursor één frame naar beneden.

Er was eens, lang voor computers, een machine die een Teletype Model heette en 10 tekens per seconde kon typen.

Het probleem was dat het 0,2 seconden duurde om een regeleinde te maken. Als er in die 0,2 seconden een nieuw teken doorkwam, zou het teken verloren gaan.

Daarom kwamen de ontwikkelaars op het idee om aan elke regel twee einde-regel tekens toe te voegen.

De ene heet een 'carriage return', waarmee de schrijfmachine de printkop op de linkerrand plaatst, en de andere heet een 'line feed', waarmee de schrijfmachine het papier één regel naar beneden laat lopen.

Dit is waar "line feed" en "carriage return" vandaan komen.

Later, toen de computer werd uitgevonden, werden deze twee concepten ook op computers toegepast. In die tijd was geheugen erg duur en sommige wetenschappers vonden het te verspillend om aan het eind van elke regel twee tekens toe te voegen, dus één was voldoende.

Dus de wereld barstte open.

Op Unix/Linux-systemen is het enige teken aan het einde van elke regel een "line feed", `\n`; op Windows-systemen is de standaard "carriage return + line feed", `\r\n`; op Mac-systemen is de standaard "carriage return " of `\r`.

Moderne tekst editors ondersteunen nu `\n` als afsluitend teken, dus `\r` is niet meer nodig.

## Opleiding aangepaste woordenboeken

Het is mogelijk om uw eigen set compressiewoordenboeken te trainen voor verschillende talen en tekstsoorten om het compressie-effect te verbeteren.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Streaming compressie nog niet geïmplementeerd

Er is geen streaming compressie toegepast (mijn scenario bestaat immers hoofdzakelijk uit korte teksten).

Iedereen die het nodig heeft, kan zelf een andere streaming compressie verpakken.

Bijvoorbeeld, comprimeer elke 1MB en noteer dan het aantal bytes van de gecomprimeerde inhoud aan het begin van elke paragraaf na de compressie.

## Coderingsprincipes

Tekst van een dozijn tot een paar honderd karakters, voornamelijk in het Chinees, is niet geschikt voor compressiealgoritmen voor algemeen gebruik.

Ik heb bijvoorbeeld [zstd](https://github.com/facebook/zstd) getest, het krachtigste compressie algoritme ter wereld, en het comprimeerde vaak 42 bytes in 62 bytes (ja, het vergrootte in plaats van te comprimeren), zelfs bij het trainen van woordenboeken (ik kon er niet achter komen hoe ik zstd zover kon krijgen om woordenboeken op te bouwen in stappen van 3 bytes; ik heb het woordenboek van zstd bekeken en het stond vol met korte zinnen).

Er zijn enkele korte tekstgerichte compressie-algoritmen, zoals [shoco](https://ed-von-schleck.github.io/shoco/) en [smaz](https://github.com/antirez/smaz), maar deze werken alleen voor Engels-achtige talen en versterken nog steeds kort Chinees (hun woordenboeken zijn slechts een paar honderd karakters lang, wat niet genoeg is, zodat zelfs herscholing van de woordenboeken niet haalbaar is).

Een andere compressiemogelijkheid is het wijzigen van de codering van de tekst.

Als u iets weet over unicode codering, zult u begrijpen dat het utf-8 coderingsschema drie bytes opslagruimte vereist voor één Chinees karakter (wat eigenlijk nogal verspillend is).

In gb18030 neemt één Chinees karakter twee bytes in beslag, waardoor 33% van de ruimte wordt bespaard. gb18030 bestrijkt echter niet alle unicode-tekens (het is slechts een deelverzameling van utf8) en kan niet worden gebruikt.

Er zijn gestandaardiseerde unicode-compressiecoderingen zoals [scsu](https://github.com/dop251/scsu)[(dat door SqlServer wordt gebruikt](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) en [utf-c](https://github.com/deNULL/utf-c).

Ik heb dit [getest](https://denull.github.io/utf-c) en het is ongeveer twee bytes per Chinees, plus een extra byte (bijv. 4 Chinezen is ongeveer 2*4+1 = 9 bytes).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Het belangrijkste is dat ik het web heb afgezocht en geen roest-implementaties voor deze twee coderingen heb gevonden.

Het schrijven van mijn eigen roest implementatie van deze coderingen is niet onmogelijk, maar het vereist een diep begrip van de codetabel intervallen van verschillende unicode talen, wat duur is om te leren.

Dus vroeg ik me af of ik een meer algemene en betere codeer- en compressie-oplossing kon maken.

Het aantal karakters in unicode is vast en bekend, en het schema unicode-13.0.0 heeft 143859 karakters [(zie hier](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Het is perfect mogelijk om de frequentie van voorkomen van elk karakter te tellen en het dan te comprimeren met behulp van Hoffman-codering.

Dus, met behulp van een aantal Chinese corpus, begon ik met het tellen van de woordfrequenties.

Het corpus ziet er als volgt uit.

* [Wikipedia Chinees corpus](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel crawler](https://github.com/ma6254/FictionDown) (de release versie zal steeds opnieuw ongeldige hoofdstukken crawlen, dus de master versie is nodig `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler voor het BT netwerk](https://github.com/gcxfd/bt-spider)
* [Een paar crawlers uit het niets geschreven, zie code spider directory](https://github.com/rmw-link/utf8_compress/tree/master/spider)

De resultaten zijn goed, drie Chinese karakters kunnen worden gecomprimeerd tot 5 bytes, wat al verder is dan de compressie van gb18030.

Verder vroeg ik me af of ik gewone woorden kon toevoegen aan het woordenboek van Hoffman om het compressie-effect verder te optimaliseren.

Dus maakte ik een woordenboek met veelgebruikte woorden (gecomprimeerd tot meer dan 500 KB) met behulp van [het trainingsalgoritme in de train directory](https://github.com/rmw-link/rmw-utf8/tree/master/train) voor woordscheiding + ngram.

Ik heb het uitgeprobeerd en het verplettert elk compressie algoritme op de markt.

Gaaf.