# rmw-utf8 - utf8-Komprimierungskodierung

Ein kurzer Textkomprimierungsalgorithmus für utf-8, optimiert für Chinesisch, basierend auf der Programmiersprache rust.

Hinweis: rmw-utf8 kann nur utf-8-Text komprimieren, es ist kein allgemeiner Algorithmus zur Binärkomprimierung.

Es gibt eine [Rust-Version](https://github.com/rmw-link/rmw-utf8) und eine [Wasm-Version](https://github.com/rmw-lib/rmw-utf8-wasm) für Javascript.

## Wie zu verwenden

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Bewertung der Komprimierungsrate

Dieser Algorithmus wurde für die Komprimierung kurzer Texte entwickelt und liefert folgende Ergebnisse Wie Sie sehen, ist die Komprimierungsrate dieses Algorithmus umso besser, je kürzer der Text ist.

Mit 22467 Bytes (etwa 7500 chinesische Zeichen) übertrifft rmw-utf8 immer noch die allgemeinen Kompressionsalgorithmen.

```
#include compress_test/test.txt
```

Das Testgerät ist ein MacBook Pro 2015 ( 2,2 GHz Intel Core i7 )

Der Testcode ist unter [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test) zu finden

## Hinweise zur Verwendung

Bei der Komprimierung werden sowohl `\r\n` als auch `\r` im Text durch `\n`ersetzt, was bedeutet, dass der komprimierte und der dekomprimierte Text nicht identisch sein müssen.

### Geschichte

`\r``\n` Ersteres bringt den Cursor an den Anfang der Zeile, letzteres bewegt den Cursor ein Bild nach unten.

Es war einmal, lange vor dem Computer, eine Maschine namens Teletype Model, die 10 Zeichen pro Sekunde tippen konnte.

Das Problem war, dass es 0,2 Sekunden dauerte, einen Zeilenumbruch zu machen. Wenn in diesen 0,2 Sekunden ein neues Zeichen auftauchen würde, wäre das Zeichen verloren.

Also kamen die Entwickler auf die Idee, jeder Zeile zwei Zeilenendezeichen hinzuzufügen.

Die eine wird als "Wagenrücklauf" bezeichnet, der die Schreibmaschine anweist, den Druckkopf am linken Rand zu positionieren, und die andere als "Zeilenvorschub", der die Schreibmaschine anweist, das Papier eine Zeile nach unten zu bewegen.

Daher kommen auch die Begriffe "Zeilenvorschub" und "Wagenrücklauf".

Später, als der Computer erfunden wurde, wurden diese beiden Konzepte auch auf Computer angewandt. Damals war der Speicher sehr teuer, und einige Wissenschaftler hielten es für zu verschwenderisch, am Ende jeder Zeile zwei Zeichen hinzuzufügen, so dass ein Zeichen genügte.

So brach die Welt auf.

Auf Unix/Linux-Systemen ist das einzige Zeichen am Ende jeder Zeile ein "Zeilenvorschub", `\n`; auf Windows-Systemen ist der Standard "Wagenrücklauf + Zeilenvorschub", `\r\n`; auf Mac-Systemen ist der Standard "Wagenrücklauf " oder `\r`.

Moderne Texteditoren unterstützen inzwischen `\n` als Schlusszeichen, so dass `\r` nicht mehr erforderlich ist.

## Training von benutzerdefinierten Wörterbüchern

Es ist möglich, eigene Komprimierungswörterbücher für verschiedene Sprachen und Textsorten zu trainieren, um den Komprimierungseffekt zu verbessern.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Streaming-Kompression noch nicht implementiert

Es wurde keine Streaming-Komprimierung vorgenommen (schließlich handelt es sich bei meinem Szenario hauptsächlich um kurze Texte).

Jeder, der es braucht, kann eine andere Streaming-Kompression selbst verpacken.

Komprimieren Sie z. B. alle 1 MB und notieren Sie dann die Anzahl der Bytes des komprimierten Inhalts am Anfang jedes Absatzes nach der Komprimierung.

## Grundsätze der Kodierung

Texte mit einem Dutzend bis ein paar hundert Zeichen, vor allem in chinesischer Sprache, sind für allgemeine Komprimierungsalgorithmen nicht geeignet.

Ich habe z.B. [zstd](https://github.com/facebook/zstd) getestet, den leistungsfähigsten Kompressionsalgorithmus der Welt, und er hat oft 42 Bytes in 62 Bytes komprimiert (ja, er hat vergrößert, statt zu komprimieren), sogar beim Training von Wörterbüchern (ich konnte nicht herausfinden, wie ich zstd dazu bringe, Wörterbücher in 3-Byte-Schritten zu erstellen; ich habe das Wörterbuch von zstd überprüft, und es war voll von kurzen Sätzen).

Es gibt einige auf kurze Texte ausgerichtete Kompressionsalgorithmen wie [shoco](https://ed-von-schleck.github.io/shoco/) und [smaz](https://github.com/antirez/smaz), aber diese funktionieren nur für englischähnliche Sprachen und verstärken immer noch kurzes Chinesisch (ihre Wörterbücher sind nur ein paar hundert Zeichen lang, was nicht ausreicht, so dass selbst eine Umschulung der Wörterbücher nicht machbar ist).

Eine weitere Möglichkeit der Komprimierung besteht darin, die Kodierung des Textes zu ändern.

Wenn Sie etwas über Unicode-Kodierung wissen, werden Sie verstehen, dass das utf-8-Kodierungsschema drei Bytes Speicherplatz für ein chinesisches Zeichen benötigt (was eigentlich ziemlich verschwenderisch ist).

In gb18030 nimmt ein chinesisches Zeichen zwei Bytes ein, was 33 % des Platzes spart. Allerdings deckt gb18030 nicht alle Unicode-Zeichen ab (es ist nur eine Teilmenge von utf8) und kann daher nicht verwendet werden.

Es gibt standardisierte Unicode-Kompressionskodierungen wie [scsu](https://github.com/dop251/scsu)[(die von SqlServer verwendet wird](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) und [utf-c](https://github.com/deNULL/utf-c).

Ich habe dies [getestet](https://denull.github.io/utf-c) und es sind etwa zwei Bytes pro Chinesisch, plus ein zusätzliches Byte (z.B. 4 Chinesisch sind etwa 2*4+1 = 9 Bytes).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Der springende Punkt ist, dass ich das Web durchsucht habe und keine Rost-Implementierungen für diese beiden Kodierungen gefunden habe.

Meine eigene Rust-Implementierung dieser Kodierungen zu schreiben ist nicht unmöglich, aber es erfordert ein tiefes Verständnis der Codetabellenintervalle verschiedener Unicode-Sprachen, was teuer zu lernen ist.

Also habe ich mich gefragt, ob ich eine allgemeinere und bessere Kodierungs- und Komprimierungslösung entwickeln könnte.

Die Anzahl der Zeichen in Unicode ist festgelegt und bekannt, und das Unicode-13.0.0-Schema hat 143859 Zeichen [(siehe hier](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Es ist durchaus möglich, die Häufigkeit des Auftretens der einzelnen Zeichen zu zählen und sie dann mit der Hoffman-Kodierung zu komprimieren.

Also habe ich anhand eines chinesischen Korpus begonnen, die Worthäufigkeiten zu zählen.

Der Korpus setzt sich wie folgt zusammen.

* [Wikipedia-Korpus Chinesisch](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown Web-Roman-Crawler](https://github.com/ma6254/FictionDown) (die Release-Version wird immer wieder ungültige Kapitel crawlen, daher wird die Master-Version benötigt `go get github.com/ma6254/FictionDown@master`)
* [Weibo-Crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT-Crawler für das BT-Netz](https://github.com/gcxfd/bt-spider)
* [Ein paar aus dem Stegreif geschriebene Crawler, siehe Code-Spider-Verzeichnis](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Die Ergebnisse sind gut, drei chinesische Zeichen können auf 5 Bytes komprimiert werden, was bereits über die Kompression von gb18030 hinausgeht.

Außerdem habe ich mich gefragt, ob ich dem Hoffman-Wörterbuch allgemeine Wörter hinzufügen könnte, um den Komprimierungseffekt weiter zu optimieren.

Also habe ich ein Wörterbuch mit häufig verwendeten Wörtern erstellt (komprimiert auf über 500 KB) und [den Trainingsalgorithmus im train-Verzeichnis](https://github.com/rmw-link/rmw-utf8/tree/master/train) für Worttrennung + Ngramm verwendet.

Ich habe es ausprobiert und es übertrifft jeden Kompressionsalgorithmus auf dem Markt.

Cool.