# rmw-utf8 - codificación de compresión utf8

Un algoritmo de compresión de texto corto para utf-8, optimizado para el chino, basado en el lenguaje de programación rust.

Nota: rmw-utf8 sólo puede comprimir texto utf-8, no es un algoritmo de compresión binaria de propósito general.

Hay una [versión rust](https://github.com/rmw-link/rmw-utf8) y una [versión wasm](https://github.com/rmw-lib/rmw-utf8-wasm) para javascript.

## Cómo utilizarlo

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Evaluación de la tasa de compresión

Este algoritmo está diseñado para comprimir textos cortos y los resultados son los siguientes. Como puede ver, cuanto más corto es el texto, mejor es la tasa de compresión de este algoritmo.

Con 22467 bytes (unos 7500 caracteres chinos), rmw-utf8 sigue superando a los algoritmos de compresión genéricos.

```
#include compress_test/test.txt
```

La máquina de prueba es un MacBook Pro 2015 ( 2,2 GHz Intel Core i7 )

El código de prueba se encuentra en [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Notas de uso

La compresión sustituye tanto `\r\n` como `\r` en el texto por `\n`, lo que significa que el texto comprimido y el descomprimido pueden no ser idénticos.

### Historia

`\r``\n` El primero lleva el cursor al principio de la línea y el segundo mueve el cursor hacia abajo un cuadro.

Érase una vez, mucho antes de los ordenadores, una máquina llamada teletipo que podía escribir 10 caracteres por segundo.

El problema era que tardaba 0,2 segundos en hacer un salto de línea. Si en esos 0,2 segundos llegara un nuevo personaje, éste se perdería.

Así que a los desarrolladores se les ocurrió añadir dos caracteres de fin de línea a cada línea.

Uno se llama "retorno de carro", que indica a la máquina de escribir que coloque el cabezal de impresión en el borde izquierdo, y el otro se llama "avance de línea", que indica a la máquina de escribir que desplace el papel una línea hacia abajo.

De ahí vienen el "salto de línea" y el "retorno de carro".

Más tarde, cuando se inventó el ordenador, estos dos conceptos se aplicaron también a las computadoras. En aquella época, la memoria era muy cara y algunos científicos pensaban que era demasiado derrochador añadir dos caracteres al final de cada línea, por lo que bastaba con uno.

Entonces el mundo se abrió.

En los sistemas Unix/Linux, el único carácter al final de cada línea es un "salto de línea", `\n`; en los sistemas Windows, el valor por defecto es "retorno de carro + salto de línea", `\r\n`; en los sistemas Mac, el valor por defecto es "retorno de carro " o `\r`.

Los editores de texto modernos admiten ahora `\n` como carácter de cierre, por lo que no es necesario `\r`.

## Formación de diccionarios personalizados

Es posible entrenar su propio conjunto de diccionarios de compresión para diferentes idiomas y tipos de texto para mejorar el efecto de compresión.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## La compresión de flujo no se ha implementado todavía

No se ha realizado ninguna compresión de flujo (después de todo, mi escenario es principalmente de textos cortos).

Cualquiera que lo necesite puede empaquetar por sí mismo otra compresión de streaming.

Por ejemplo, comprime cada 1MB y luego registra el número de bytes de contenido comprimido al principio de cada párrafo después de la compresión.

## Principios de codificación

Los textos de una docena a un par de cientos de caracteres, principalmente en chino, no son adecuados para los algoritmos de compresión de uso general.

Por ejemplo, probé [zstd](https://github.com/facebook/zstd), el algoritmo de compresión más potente del mundo, y a menudo comprimía 42 bytes en 62 bytes (sí, ampliaba en lugar de comprimir), incluso cuando entrenaba diccionarios (no pude averiguar cómo hacer que zstd construyera diccionarios en incrementos de 3 bytes; caté el diccionario de zstd y estaba lleno de frases cortas).

Existen algunos algoritmos de compresión orientados a textos cortos, como [shoco](https://ed-von-schleck.github.io/shoco/) y [smaz](https://github.com/antirez/smaz), pero sólo funcionan para idiomas similares al inglés y siguen amplificando el chino corto (sus diccionarios sólo tienen unos cientos de caracteres, lo que no es suficiente, por lo que ni siquiera es factible volver a entrenar los diccionarios).

Otra opción de compresión es cambiar la codificación del texto.

Si sabe algo sobre codificación unicode, comprenderá que el esquema de codificación utf-8 requiere tres bytes de espacio de almacenamiento para un carácter chino (lo que en realidad es un gran desperdicio).

En gb18030, un carácter chino ocupa dos bytes, ahorrando un 33% de espacio. Sin embargo, gb18030 no cubre todos los caracteres unicode (sólo es un subconjunto de utf8) y no puede utilizarse.

Existen codificaciones de compresión unicode estandarizadas como [scsu](https://github.com/dop251/scsu)[(que utiliza SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) y [utf-c](https://github.com/deNULL/utf-c).

Lo he [probado](https://denull.github.io/utf-c) y son unos dos bytes por chino, más un byte extra (por ejemplo, 4 chinos son unos 2*4+1 = 9 bytes).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Lo fundamental es que he buscado en la web y no he encontrado implementaciones de óxido para estas dos codificaciones.

Escribir mi propia implementación en rust de estas codificaciones no es imposible, pero requiere un profundo conocimiento de los intervalos de la tabla de códigos de varios lenguajes unicode, lo cual es costoso de aprender.

Así que me pregunté si podría hacer una solución de codificación y compresión más general y mejor.

El número de caracteres en unicode es fijo y conocido, y el esquema unicode-13.0.0 tiene 143859 caracteres [(ver aquí](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Es perfectamente posible contar la frecuencia de aparición de cada carácter y luego comprimirlo utilizando la codificación Hoffman.

Así que, utilizando un corpus chino, empecé a contar las frecuencias de las palabras.

El corpus es el siguiente.

* [Corpus chino de Wikipedia](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* El [rastreador de novelas web FictionDown](https://github.com/ma6254/FictionDown) (la versión de lanzamiento rastreará repetidamente los capítulos no válidos una y otra vez, por lo que se necesita la versión maestra `go get github.com/ma6254/FictionDown@master`)
* [Rastreador de Weibo](https://github.com/gcxfd/weibo-crawler)
* [Rastreador DHT para la red BT](https://github.com/gcxfd/bt-spider)
* [Algunos rastreadores escritos de improviso, véase el directorio de arañas de código](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Los resultados son buenos, tres caracteres chinos se pueden comprimir a 5 bytes, lo que ya supera la compresión de gb18030.

Además, me pregunté si podría añadir palabras comunes al diccionario de Hoffman para optimizar aún más el efecto de compresión.

Así que hice un diccionario con palabras de uso común (comprimido a más de 500 KB) usando [el algoritmo de entrenamiento en el directorio train](https://github.com/rmw-link/rmw-utf8/tree/master/train) para la separación de palabras + ngrama.

Lo he probado y aplasta a todos los algoritmos de compresión del mercado.

Genial.