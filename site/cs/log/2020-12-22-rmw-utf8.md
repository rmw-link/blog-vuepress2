# rmw-utf8 - kompresní kódování utf8

Algoritmus pro kompresi krátkého textu utf-8, optimalizovaný pro čínštinu, založený na programovacím jazyce rust.

Poznámka: rmw-utf8 umí komprimovat pouze text ve formátu utf-8, nejedná se o univerzální binární kompresní algoritmus.

Existuje [verze rust](https://github.com/rmw-link/rmw-utf8) a [verze wasm](https://github.com/rmw-lib/rmw-utf8-wasm) pro javascript.

## Jak používat

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Hodnocení míry komprese

Tento algoritmus je určen ke kompresi krátkých textů a jeho výsledky jsou následující. Jak vidíte, čím kratší je text, tím lepší je míra komprese tohoto algoritmu.

Při velikosti 22467 bajtů (přibližně 7500 čínských znaků) rmw-utf8 stále překonává obecné kompresní algoritmy.

```
#include compress_test/test.txt
```

Testovacím počítačem je MacBook Pro 2015 ( 2,2 GHz Intel Core i7 ).

Testovací kód naleznete na adrese [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Poznámky k použití

Komprese nahrazuje v textu `\r\n` i `\r` pomocí `\n`, což znamená, že komprimovaný a dekomprimovaný text nemusí být totožný.

### Historie

`\r``\n` První z nich přenese kurzor na začátek řádku a druhý přesune kurzor o jeden snímek dolů.

Kdysi dávno před počítači existoval stroj zvaný Teletype Model, který dokázal napsat 10 znaků za sekundu.

Problém spočíval v tom, že vytvoření zlomu řádku trvalo 0,2 sekundy. Pokud by se během těchto 0,2 sekundy objevila nová postava, byla by ztracena.

Vývojáři proto přišli s nápadem přidat na každý řádek dva znaky konce řádku.

Jeden z nich se nazývá "carriage return", který říká psacímu stroji, aby umístil tiskovou hlavu na levý okraj, a druhý se nazývá "line feed", který říká psacímu stroji, aby posunul papír o jeden řádek dolů.

Odtud pochází "line feed" a "carriage return".

Později, když byl vynalezen počítač, byly tyto dva pojmy aplikovány i na počítače. V té době byla paměť velmi drahá a někteří vědci si mysleli, že přidávat na konec každého řádku dva znaky je příliš neekonomické, takže stačil jeden.

Svět se otevřel.

V systémech Unix/Linux je jediným znakem na konci každého řádku "line feed", `\n`; v systémech Windows je výchozí "carriage return + line feed", `\r\n`; v systémech Mac je výchozí "carriage return". " nebo `\r`.

Moderní textové editory nyní podporují `\n` jako uzavírací znak, takže není třeba používat `\r`.

## Školení vlastních slovníků

Je možné trénovat vlastní sadu kompresních slovníků pro různé jazyky a typy textů a zvýšit tak kompresní efekt.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Komprese datových proudů zatím není implementována

Nebyla provedena žádná komprese datových toků (koneckonců můj scénář se týká především krátkých textů).

Každý, kdo to potřebuje, si může sám zabalit další kompresi datového toku.

Například komprimujte každý 1 MB a po kompresi na začátku každého odstavce zaznamenejte počet bajtů komprimovaného obsahu.

## Zásady kódování

Text o délce desítek až několika set znaků, především v čínštině, není vhodný pro univerzální kompresní algoritmy.

Například jsem testoval [zstd](https://github.com/facebook/zstd), nejvýkonnější kompresní algoritmus na světě, a ten často komprimoval 42 bajtů do 62 bajtů (ano, místo komprese zvětšoval), a to i při trénování slovníků (nemohl jsem přijít na to, jak přimět zstd, aby sestavoval slovníky po 3 bajtech; zkontroloval jsem slovník zstd a byl plný krátkých vět).

Existují některé kompresní algoritmy zaměřené na krátké texty, například [shoco](https://ed-von-schleck.github.io/shoco/) a [smaz](https://github.com/antirez/smaz), ale ty fungují pouze pro jazyky podobné angličtině a stále zesilují krátkou čínštinu (jejich slovníky mají jen několik set znaků, což je málo, takže ani přeškolení slovníků není proveditelné).

Další možností komprese je změna kódování textu.

Pokud víte něco o kódování unicode, pochopíte, že kódovací schéma utf-8 vyžaduje tři bajty úložného prostoru pro jeden čínský znak (což je vlastně docela zbytečné).

V gb18030 zabírá jeden čínský znak dva bajty, čímž se ušetří 33 % místa. Jazyk gb18030 však nepokrývá všechny znaky Unicode (je pouze podmnožinou utf8) a nelze jej použít.

Existují standardizovaná kompresní kódování unicode, například [scsu](https://github.com/dop251/scsu) (které [používá SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) a [utf-c](https://github.com/deNULL/utf-c).

[Testoval](https://denull.github.io/utf-c) jsem to a na jednu čínštinu připadají asi dva bajty plus jeden bajt navíc (např. 4 čínštiny jsou asi 2*4+1 = 9 bajtů).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Klíčové je, že jsem prohledal web a nenašel jsem žádné implementace rustu pro tato dvě kódování.

Napsat vlastní implementaci těchto kódování v jazyce rust není nemožné, ale vyžaduje to hlubokou znalost intervalů kódových tabulek různých unicode jazyků, což je nákladné se naučit.

Tak jsem si říkal, jestli bych nemohl vytvořit obecnější a lepší řešení kódování a komprese.

Počet znaků v unicode je pevně daný a známý, schéma unicode-13.0.0 má 143859 znaků [(viz zde)](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py).

Je možné spočítat četnost výskytu jednotlivých znaků a následně je komprimovat pomocí Hoffmanova kódování.

Proto jsem začal počítat četnost slov v čínském korpusu.

Korpus je následující.

* [Čínský korpus Wikipedie](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel crawler](https://github.com/ma6254/FictionDown) (release verze opakovaně prochází neplatné kapitoly, takže je potřeba master verze `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler pro síť BT](https://github.com/gcxfd/bt-spider)
* [Několik crawlerů napsaných z ruky, viz adresář s kódem pavouka](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Výsledky jsou dobré, tři čínské znaky lze zkomprimovat na 5 bajtů, což je již za hranicí komprese gb18030.

Dále mě zajímalo, zda bych mohl do Hoffmanova slovníku přidat běžná slova, abych ještě více optimalizoval kompresní efekt.

Vytvořil jsem tedy slovník s běžně používanými slovy (komprimovaný na více než 500 KB) pomocí [tréninkového algoritmu v adresáři train](https://github.com/rmw-link/rmw-utf8/tree/master/train) pro separaci slov + ngram.

Vyzkoušel jsem ho a překonává všechny kompresní algoritmy na trhu.

Skvělé.