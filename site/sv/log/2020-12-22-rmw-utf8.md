# rmw-utf8 - kodning av utf8-komprimering

En algoritm för kort textkomprimering för utf-8, optimerad för kinesiska, baserad på programmeringsspråket rust.

Observera: rmw-utf8 kan endast komprimera utf-8-text, det är inte en binär komprimeringsalgoritm för allmänna ändamål.

Det finns en [rust-](https://github.com/rmw-link/rmw-utf8) och en [wasm-version](https://github.com/rmw-lib/rmw-utf8-wasm) för javascript.

## Hur man använder

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Utvärdering av kompressionshastigheten

Denna algoritm är utformad för att komprimera korta texter och resultaten är följande. Som du kan se, ju kortare texten är, desto bättre komprimeringsgrad för denna algoritm.

Med 22467 bytes (cirka 7500 kinesiska tecken) presterar rmw-utf8 fortfarande bättre än generiska komprimeringsalgoritmer.

```
#include compress_test/test.txt
```

Testmaskinen är en MacBook Pro 2015 ( 2,2 GHz Intel Core i7 )

Testkoden finns på [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Anmärkningar om användning

Komprimering ersätter både `\r\n` och `\r` i texten med `\n`, vilket innebär att den komprimerade och den dekomprimerade texten kanske inte är identiska.

### Historia

`\r``\n` Den förstnämnda förflyttar markören till början av raden och den sistnämnda flyttar markören en bildruta nedåt.

En gång i tiden, långt före datorerna, fanns det en maskin som kallades Teletype Model och som kunde skriva 10 tecken i sekunden.

Problemet var att det tog 0,2 sekunder att bryta en rad. Om en ny karaktär kom in under dessa 0,2 sekunder skulle karaktären gå förlorad.

Utvecklarna kom därför på idén att lägga till två sluttecken på varje rad.

Den ena kallas "carriage return" och säger åt skrivmaskinen att placera skrivhuvudet vid vänster kant, och den andra kallas "line feed" och säger åt skrivmaskinen att flytta pappret en rad nedåt.

Det är här som "line feed" och "carriage return" kommer ifrån.

Senare, när datorn uppfanns, tillämpades dessa två begrepp också på datorer. På den tiden var minnet mycket dyrt och vissa forskare ansåg att det var för slösaktigt att lägga till två tecken i slutet av varje rad, så det räckte med ett.

Så världen sprack upp.

På Unix/Linux-system är det enda tecknet i slutet av varje rad en "radmatning", `\n`; på Windows-system är standardvärdet "vagnretur + radmatning", `\r\n`; på Mac-system är standardvärdet "vagnretur". " eller `\r`.

Moderna textredigerare har numera stöd för `\n` som avslutande tecken, så det finns inget behov av `\r`.

## Utbildning av anpassade ordböcker

Det är möjligt att träna en egen uppsättning komprimeringsordböcker för olika språk och texttyper för att förbättra komprimeringseffekten.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Streamingkomprimering har ännu inte genomförts

Ingen strömningskomprimering har gjorts (mitt scenario består trots allt huvudsakligen av korta texter).

Den som behöver det kan själv paketera en annan streamingkomprimering.

Komprimera till exempel varje 1 MB och registrera sedan antalet bytes komprimerat innehåll i början av varje stycke efter komprimeringen.

## Principer för kodning

Text med ett dussin till ett par hundra tecken, huvudsakligen på kinesiska, lämpar sig inte för allmänna komprimeringsalgoritmer.

Jag testade till exempel [zstd](https://github.com/facebook/zstd), världens mest kraftfulla komprimeringsalgoritm, och den komprimerade ofta 42 byte till 62 byte (ja, den förstorade i stället för att komprimera), även när den tränade ordböcker (jag kunde inte komma på hur jag skulle få zstd att bygga upp ordböcker i 3-byte steg; jag kollade zstds ordbok och den var full av korta meningar).

Det finns några algoritmer för komprimering av korta texter, t.ex. [shoco](https://ed-von-schleck.github.io/shoco/) och [smaz](https://github.com/antirez/smaz), men de fungerar bara för engelskspråkiga språk och förstärker fortfarande kort kinesisk text (deras ordböcker är bara några hundra tecken långa, vilket inte räcker, så det är inte ens möjligt att omskola ordböckerna).

Ett annat komprimeringsalternativ är att ändra textens kodning.

Om du vet något om unicode-kodning förstår du att utf-8-kodningssystemet kräver tre byte lagringsutrymme för ett kinesiskt tecken (vilket faktiskt är ganska slösaktigt).

I gb18030 tar ett kinesiskt tecken upp två bytes, vilket sparar 33 % av utrymmet. gb18030 täcker dock inte alla Unicode-tecken (det är bara en delmängd av utf8) och kan därför inte användas.

Det finns standardiserade komprimeringskodningar för Unicode, t.ex. [scsu](https://github.com/dop251/scsu)[(som används av SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) och [utf-c](https://github.com/deNULL/utf-c).

Jag har [testat](https://denull.github.io/utf-c) detta och det är ungefär två byte per kinesiska, plus en extra byte (t.ex. 4 kinesiska är ungefär 2*4+1 = 9 byte).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Det viktigaste är att jag har sökt på nätet och inte hittat några rostimplementationer för dessa två kodningar.

Det är inte omöjligt att skriva en egen rostimplementering av dessa kodningar, men det kräver en djup förståelse för kodtabellintervallerna i olika Unicode-språk, vilket är dyrt att lära sig.

Så jag undrade om jag kunde göra en mer allmän och bättre lösning för kodning och komprimering.

Antalet tecken i unicode är fast och känt, och unicode-13.0.0.0 har 143859 tecken ( [se här](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Det är fullt möjligt att räkna hur ofta varje tecken förekommer och sedan komprimera det med Hoffman-kodning.

Med hjälp av en kinesisk korpus började jag räkna ordfrekvenser.

Korpusen är följande.

* [Wikipedia kinesisk korpus](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown webroman crawler](https://github.com/ma6254/FictionDown) (versionen av versionen som släpptes kommer att krypa igenom ogiltiga kapitel om och om igen, så masterversionen behövs `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT-crawler för BT-nätet](https://github.com/gcxfd/bt-spider)
* [Några crawlers som skrivits utan vidare, se katalogen för kodspindlar](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Resultaten är goda, tre kinesiska tecken kan komprimeras till 5 bytes, vilket redan är mer än komprimeringen av gb18030.

Jag funderade också på om jag kunde lägga till vanliga ord i Hoffmans ordbok för att ytterligare optimera kompressionseffekten.

Så jag gjorde en ordbok med vanligt förekommande ord (komprimerad till över 500 KB) med hjälp av [träningsalgoritmen i train-katalogen](https://github.com/rmw-link/rmw-utf8/tree/master/train) för ordseparation + ngram.

Jag testade den och den krossar alla kompressionsalgoritmer på marknaden.

Coolt.