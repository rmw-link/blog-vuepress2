# rmw-utf8 - utf8-pakkauskoodaus

Lyhyt tekstin pakkausalgoritmi utf-8:lle, optimoitu kiinan kielelle, joka perustuu rust-ohjelmointikieleen.

Huomautus: rmw-utf8 voi pakata vain utf-8-tekstiä, se ei ole yleiskäyttöinen binääripakkausalgoritmi.

Javascriptille on olemassa [rust-versio](https://github.com/rmw-link/rmw-utf8) ja [wasm-versio](https://github.com/rmw-lib/rmw-utf8-wasm).

## Kuinka käyttää

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Puristusasteen arviointi

Tämä algoritmi on suunniteltu pakkaamaan lyhyitä tekstejä, ja tulokset ovat seuraavat. Kuten näet, mitä lyhyempi teksti on, sitä parempi on tämän algoritmin pakkausteho.

22467 tavun (noin 7500 kiinalaista merkkiä) kohdalla rmw-utf8 on edelleen parempi kuin yleiset pakkausalgoritmit.

```
#include compress_test/test.txt
```

Testikoneena on MacBook Pro 2015 ( 2,2 GHz Intel Core i7 ).

Testikoodi löytyy osoitteesta [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Käyttöä koskevat huomautukset

Pakkaaminen korvaa sekä `\r\n` että `\r` tekstissä `\n`:lla, joten pakattu ja purettu teksti eivät välttämättä ole identtisiä.

### Historia

`\r``\n` Ensin mainittu tuo kursorin rivin alkuun ja jälkimmäinen siirtää kursorin yhden ruudun alaspäin.

Kauan ennen tietokoneita oli aikoinaan kone nimeltä Teletype Model, joka pystyi kirjoittamaan 10 merkkiä sekunnissa.

Ongelmana oli, että rivinvaihdon tekeminen kesti 0,2 sekuntia. Jos uusi hahmo tulisi tuon 0,2 sekunnin aikana, hahmo menetettäisiin.

Niinpä kehittäjät keksivät lisätä joka riville kaksi rivin loppumerkkiä.

Toista kutsutaan 'carriage return'-merkinnäksi, joka käskee kirjoituskonetta asettamaan tulostuspään vasempaan reunaan, ja toista 'line feed'-merkinnäksi, joka käskee kirjoituskonetta siirtämään paperia yhden rivin alaspäin.

Tästä tulevat "line feed" ja "carriage return".

Myöhemmin, kun tietokone keksittiin, näitä kahta käsitettä sovellettiin myös tietokoneisiin. Siihen aikaan muisti oli hyvin kallista, ja jotkut tutkijat pitivät kahden merkin lisäämistä jokaisen rivin loppuun liian tuhlaavana, joten yksi merkki riitti.

Niinpä maailma murtui.

Unix/Linux-järjestelmissä ainoa merkki jokaisen rivin lopussa on "line feed", `\n`; Windows-järjestelmissä oletusarvo on "carriage return + line feed", `\r\n`; Mac-järjestelmissä oletusarvo on "carriage return". " tai `\r`.

Nykyaikaiset tekstinkäsittelyohjelmat tukevat nykyään `\n` -osoitetta lopetusmerkkinä, joten `\r` -osoitetta ei tarvita.

## Mukautettujen sanakirjojen koulutus

On mahdollista kouluttaa omia pakkaussanakirjoja eri kielille ja tekstityypeille pakkaustehon tehostamiseksi.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Streaming-pakkausta ei ole vielä toteutettu

Mitään suoratoistopakkausta ei ole tehty (skenaarioni koostuu pääasiassa lyhyistä teksteistä).

Jokainen, joka tarvitsee sitä, voi itse pakata toisen suoratoistopakkauksen.

Pakkaa esimerkiksi 1 Mt:n välein ja kirjaa pakkauksen jälkeen jokaisen kappaleen alkuun pakatun sisällön tavujen määrä.

## Koodausperiaatteet

Kymmenestä pariin sataan merkkiä käsittävä teksti, joka on pääasiassa kiinankielistä, ei sovellu yleiskäyttöisille pakkausalgoritmeille.

Testasin esimerkiksi [zstd:tä](https://github.com/facebook/zstd), maailman tehokkainta pakkausalgoritmia, ja se pakasi usein 42 tavua 62 tavuksi (kyllä, se suurensi sen sijaan, että olisi pakannut), jopa sanakirjoja harjoitellessa (en saanut selville, miten saisin zstd:n rakentamaan sanakirjoja kolmen tavun askelin; katsoin zstd:n sanakirjaa ja se oli täynnä lyhyitä lauseita).

On olemassa joitakin lyhyen tekstin pakkausalgoritmeja, kuten [shoco](https://ed-von-schleck.github.io/shoco/) ja [smaz](https://github.com/antirez/smaz), mutta ne toimivat vain englanninkielisille kielille ja vahvistavat edelleen lyhyttä kiinaa (niiden sanakirjat ovat vain muutaman sadan merkin pituisia, mikä ei riitä, joten edes sanakirjojen uudelleenkouluttaminen ei ole mahdollista).

Toinen pakkausvaihtoehto on muuttaa tekstin koodausta.

Jos tiedät jotain unicode-koodauksesta, ymmärrät, että utf-8-koodausjärjestelmä vaatii kolme tavua tallennustilaa yhtä kiinalaista merkkiä varten (mikä on itse asiassa melko tuhlausta).

gb18030:ssä yksi kiinalainen merkki vie kaksi tavua, mikä säästää 33 % tilasta. gb18030 ei kuitenkaan kata kaikkia unicode-merkkejä (se on vain utf8:n osajoukko), eikä sitä voida käyttää.

On olemassa standardoituja unicode-pakkauskoodauksia, kuten [scsu](https://github.com/dop251/scsu)[(jota SqlServer käyttää](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) ja [utf-c](https://github.com/deNULL/utf-c).

Olen [testannut](https://denull.github.io/utf-c) tätä, ja se on noin kaksi tavua per kiinalainen plus yksi ylimääräinen tavu (esim. 4 kiinalaista on noin 2*4+1 = 9 tavua).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Tärkeintä on, että olen etsinyt verkosta ja löytänyt mitään ruostetoteutuksia näille kahdelle koodaukselle.

Oman rust-toteutuksen kirjoittaminen näille koodauksille ei ole mahdotonta, mutta se edellyttää eri unicode-kielten kooditaulukoiden väleihin syvällistä perehtymistä, mikä on kallista oppia.

Joten mietin, voisinko tehdä yleisemmän ja paremman koodaus- ja pakkausratkaisun.

Unicoden merkkien määrä on kiinteä ja tunnettu, ja unicode-13.0.0-järjestelmässä on 143859 merkkiä [(katso tästä](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

On täysin mahdollista laskea kunkin merkin esiintymistiheys ja sitten pakata se Hoffman-koodauksella.

Aloitin siis kiinalaisen korpuksen avulla sanataajuuksien laskemisen.

Korpus on seuraava.

* [Wikipedian kiinalainen korpus](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown-verkkoromaanien indeksointiohjelma](https://github.com/ma6254/FictionDown) (julkaisuversio indeksoi toistuvasti virheellisiä lukuja uudestaan ja uudestaan, joten master-versiota tarvitaan `go get github.com/ma6254/FictionDown@master`).
* [Weibo indeksoija](https://github.com/gcxfd/weibo-crawler)
* [BT-verkon DHT-matkaaja](https://github.com/gcxfd/bt-spider)
* [Muutama indeksoijia kirjoitettu off the cuff, katso koodi hämähäkki hakemisto](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Tulokset ovat hyviä, kolme kiinalaista merkkiä voidaan pakata 5 tavuun, mikä on jo yli gb18030:n pakkauksen.

Lisäksi mietin, voisinko lisätä Hoffmanin sanakirjaan yleisiä sanoja, jotta pakkausvaikutusta voitaisiin optimoida entisestään.

Tein siis sanakirjan yleisesti käytetyistä sanoista (pakattuna yli 500 kilotavuun) käyttäen [train-hakemistossa olevaa harjoittelualgoritmia](https://github.com/rmw-link/rmw-utf8/tree/master/train) sanojen erottelua + ngrammia varten.

Kokeilin sitä, ja se murskaa kaikki markkinoilla olevat pakkausalgoritmit.

Hienoa.