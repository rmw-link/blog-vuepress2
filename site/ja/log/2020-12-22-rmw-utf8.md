# rmw-utf8 - utf8 圧縮エンコーディング

プログラミング言語Rustをベースにした、中国語に最適化されたutf-8の短文テキスト圧縮アルゴリズム。

注意：rmw-utf8はutf-8テキストのみを圧縮することができ、汎用のバイナリ圧縮アルゴリズムではありません。

javascriptには、 [rust版と](https://github.com/rmw-link/rmw-utf8) [wasm](https://github.com/rmw-lib/rmw-utf8-wasm)版があります。

## 使用方法

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## 圧縮率評価

このアルゴリズムは、短いテキストを圧縮するために設計されており、結果は次のようになります。ご覧のように、このアルゴリズムでは、テキストが短いほど圧縮率が高くなります。

22467バイト（約7500漢字）で、rmw-utf8は依然として一般的な圧縮アルゴリズムより優れている。

```
#include compress_test/test.txt
```

テスト機はMacBook Pro 2015 ( 2.2GHz Intel Core i7 ) です。

テストコードは [compress_testに](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)あります。

## 使用上の注意点

圧縮すると、テキスト中の `\r\n` と `\r` の両方が `\n`に置き換えられます。つまり、圧縮と解凍されたテキストは同一でない可能性があります。

### 沿革

`\r``\n` 前者はカーソルを行頭に移動させ、後者はカーソルを1フレーム下に移動させます。

昔々、コンピュータが登場するずっと前に、1秒間に10文字打てるテレタイプモデルという機械があった。

問題は、改行するのに0.2秒かかることだった。その0.2秒の間に新しいキャラクターが来たら、そのキャラクターは消えてしまうのです。

そこで開発者は、1行に2文字ずつ行末をつけることを思いついた。

一つは「キャリッジリターン」と呼ばれるもので、タイプライターの印字ヘッドを左の境界線に配置するように指示し、もう一つは「ラインフィード」と呼ばれるもので、タイプライターに用紙を1行下に移動させるように指示するものである。

改行」「復帰」はここから来ている。

その後、コンピュータが発明されると、この2つの概念はコンピュータにも適用されるようになった。当時はメモリが非常に高価で、行末に2文字ずつつけるのはもったいないから1文字でいいという科学者もいました。

それで、世界が割れたんです。

Unix/Linuxシステムでは，各行の末尾の文字は「ラインフィード」（ `\n`）のみです。Windowsシステムでは，デフォルトは「キャリッジリターン＋ラインフィード」（ `\r\n`），Macシステムでは，デフォルトは「キャリッジリターン」（ ）です。"または `\r`。

最近のテキストエディタは、閉じ文字として `\n` をサポートしているので、 `\r` は必要ありません。

## カスタム辞書のトレーニング

言語やテキストの種類に応じた独自の圧縮辞書を学習させ、圧縮効果を高めることが可能です。

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## ストリーミング圧縮は未実装

ストリーミング圧縮は行っていません（なにしろ、私のシナリオは短文が中心ですから）。

必要な人は自分で別のストリーミング圧縮をパッケージングすることができます。

例えば、1MBごとに圧縮し、圧縮後の各段落の冒頭に圧縮したコンテンツのバイト数を記録する。

## 符号化の原則

中国語を中心とした十数文字から数百文字のテキストは、汎用の圧縮アルゴリズムには適さない。

例えば、世界で最も強力な圧縮アルゴリズムである [zstdを](https://github.com/facebook/zstd)テストしたところ、辞書の学習時でさえ、42バイトを62バイトに圧縮する（そう、圧縮する代わりに拡大する）ことが多かった（zstdに3バイト単位の辞書を構築させる方法がわからなかった。zstdの辞書をキャッチングしたら、短い文ばかりだった）。

短文指向の圧縮アルゴリズムとしては、 [shocoや](https://ed-von-schleck.github.io/shoco/)[smazなどが](https://github.com/antirez/smaz)あるが、これらは英語に近い言語にしか使えないため、やはり短い中国語を増幅してしまう（辞書の長さが数百文字と短く、辞書の再教育すらままならない）。

もう一つの圧縮方法は、テキストのエンコーディングを変更することです。

unicodeエンコーディングについてご存知の方なら、utf-8エンコーディング方式が1つの漢字に対して3バイトの記憶領域を必要とすることをご理解いただけるでしょう（これは実際には非常に無駄なことなのですが）。

gb18030では、漢字1文字が2バイトで済むため、33%の容量を節約することができます。ただし、gb18030はすべてのunicode文字をカバーしているわけではなく（utf8のサブセットに過ぎない）、使用することはできません。

標準化されたユニコード圧縮エンコーディングとして、 [scsu](https://github.com/dop251/scsu)[（SqlServerで](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15)使用されている）や[utf-cが](https://github.com/deNULL/utf-c)あります。

私が [テスト](https://denull.github.io/utf-c)したところ、中国語1つにつき約2バイト、さらに1バイト追加されます（例：中国語4つで約2*4+1=9バイト）。

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

重要なのは、Webを検索しても、この2つのエンコーディングのRust実装がないことです。

これらのエンコーディングのRust実装を自分で書くことは不可能ではありませんが、様々なユニコード言語のコード表間隔を深く理解する必要があり、その習得にはコストがかかります。

そこで、もっと汎用的で優れたエンコードや圧縮のソリューションが作れないかと考えたのです。

unicodeの文字数は固定で既知であり、unicode-13.0.0スキームでは143859文字である[（こちらを参照](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py)）。

各文字の出現頻度をカウントして、Hoffmanエンコーディングで圧縮することは完全に可能である。

そこで、いくつかの中国語コーパスを用いて、単語の出現頻度をカウントし始めました。

コーパスは以下の通りです。

* [ウィキペディア中国語コーパス](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown Web小説クローラー](https://github.com/ma6254/FictionDown)（リリース版では無効な章を何度もクロールするため、マスター版が必要です `go get github.com/ma6254/FictionDown@master`)
* [ウェイボー・クローラー](https://github.com/gcxfd/weibo-crawler)
* [BTネットワーク用DHTクローラー](https://github.com/gcxfd/bt-spider)
* [クローラーを数種類、即席で作成しました。](https://github.com/rmw-link/utf8_compress/tree/master/spider)

結果は良好で、漢字3文字を5バイトに圧縮でき、すでにgb18030の圧縮率を超えている。

さらに、ホフマンの辞書によく使われる単語を追加して、圧縮効果をさらに最適化できないかと考えました。

そこで、 [trainディレクトリにある](https://github.com/rmw-link/rmw-utf8/tree/master/train) 単語分離＋ngramの [学習アルゴリズム](https://github.com/rmw-link/rmw-utf8/tree/master/train) を使って、よく使う単語を集めた辞書（500KB以上に圧縮）を作りました。

実際に試してみたところ、市場に出回っているあらゆる圧縮アルゴリズムを粉砕していました。

かっこいい。