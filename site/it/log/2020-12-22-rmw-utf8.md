# rmw-utf8 - codifica di compressione utf8

Un breve algoritmo di compressione del testo per utf-8, ottimizzato per il cinese, basato sul linguaggio di programmazione rust.

Nota: rmw-utf8 può solo comprimere il testo utf-8, non è un algoritmo di compressione binaria di uso generale.

C'è una [versione rust](https://github.com/rmw-link/rmw-utf8) e una [versione wasm](https://github.com/rmw-lib/rmw-utf8-wasm) per javascript.

## Come si usa

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Valutazione del tasso di compressione

Questo algoritmo è progettato per comprimere testi brevi e i risultati sono i seguenti. Come potete vedere, più breve è il testo, migliore è il tasso di compressione di questo algoritmo.

A 22467 byte (circa 7500 caratteri cinesi), rmw-utf8 supera ancora gli algoritmi di compressione generici.

```
#include compress_test/test.txt
```

La macchina di prova è un MacBook Pro 2015 (2.2 GHz Intel Core i7)

Il codice del test può essere trovato in [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Note sull'uso

La compressione sostituisce sia `\r\n` che `\r` nel testo con `\n`, il che significa che il testo compresso e decompresso potrebbe non essere identico.

### Storia

`\r``\n` Il primo porta il cursore all'inizio della linea e il secondo sposta il cursore in basso di un fotogramma.

C'era una volta, molto prima dei computer, una macchina chiamata Teletype Model che poteva digitare 10 caratteri al secondo.

Il problema era che ci volevano 0,2 secondi per fare un'interruzione di linea. Se un nuovo personaggio arrivasse in quei 0,2 secondi, il personaggio sarebbe perso.

Così gli sviluppatori hanno avuto l'idea di aggiungere due caratteri di fine linea ad ogni linea.

Uno è chiamato 'ritorno a capo', che dice alla macchina da scrivere di posizionare la testina di stampa sul bordo sinistro, e l'altro è chiamato 'avanzamento riga', che dice alla macchina da scrivere di spostare la carta giù di una riga.

È da qui che provengono "line feed" e "carriage return".

Più tardi, quando fu inventato il computer, questi due concetti furono applicati anche ai computer. A quel tempo, la memoria era molto costosa e alcuni scienziati pensavano che fosse troppo dispendioso aggiungere due caratteri alla fine di ogni riga, quindi ne bastava uno.

Così il mondo si è spaccato.

Sui sistemi Unix/Linux, l'unico carattere alla fine di ogni linea è un "line feed", `\n`; sui sistemi Windows, il default è "carriage return + line feed", `\r\n`; sui sistemi Mac, il default è "carriage return " o `\r`.

I moderni editor di testo ora supportano `\n` come carattere di chiusura, quindi non c'è bisogno di `\r`.

## Formazione di dizionari personalizzati

È possibile addestrare il proprio set di dizionari di compressione per diverse lingue e tipi di testo per migliorare l'effetto di compressione.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Compressione dello streaming non ancora implementata

Non è stata fatta alcuna compressione dello streaming (dopo tutto, il mio scenario è principalmente di testi brevi).

Chiunque ne abbia bisogno può confezionare un'altra compressione di streaming da solo.

Per esempio, comprimi ogni 1MB e poi registra il numero di byte di contenuto compresso all'inizio di ogni paragrafo dopo la compressione.

## Principi di codifica

I testi da una dozzina a un paio di centinaia di caratteri, principalmente in cinese, non sono adatti agli algoritmi di compressione di uso generale.

Per esempio, ho testato [zstd](https://github.com/facebook/zstd), l'algoritmo di compressione più potente del mondo, e spesso comprimeva 42 byte in 62 byte (sì, ingrandiva invece di comprimere), anche quando addestrava i dizionari (non sono riuscito a capire come far costruire a zstd dizionari con incrementi di 3 byte; ho catturato il dizionario di zstd ed era pieno di frasi brevi).

Ci sono alcuni algoritmi di compressione orientati al testo breve, come [shoco](https://ed-von-schleck.github.io/shoco/) e [smaz](https://github.com/antirez/smaz), ma questi funzionano solo per lingue simili all'inglese e amplificano ancora il cinese breve (i loro dizionari sono lunghi solo poche centinaia di caratteri, il che non è sufficiente, quindi anche la riqualificazione dei dizionari non è fattibile).

Un'altra opzione di compressione è cambiare la codifica del testo.

Se sapete qualcosa sulla codifica unicode, capirete che lo schema di codifica utf-8 richiede tre byte di spazio di archiviazione per un carattere cinese (che in realtà è abbastanza sprecato).

In gb18030, un carattere cinese occupa due byte, risparmiando il 33% dello spazio. Tuttavia, gb18030 non copre tutti i caratteri unicode (è solo un sottoinsieme di utf8) e non può essere usato.

Ci sono codifiche di compressione unicode standardizzate come [scsu](https://github.com/dop251/scsu)[(che è usato da SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) e [utf-c](https://github.com/deNULL/utf-c).

L'ho [testato](https://denull.github.io/utf-c) ed è circa due byte per cinese, più un byte extra (ad esempio 4 cinesi sono circa 2*4+1 = 9 byte).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

La cosa fondamentale è che ho cercato sul web e non ho trovato implementazioni di ruggine per queste due codifiche.

Scrivere la mia implementazione rust di queste codifiche non è impossibile, ma richiede una profonda comprensione degli intervalli delle tabelle di codice dei vari linguaggi unicode, che è costoso da imparare.

Così mi sono chiesto se potevo fare una soluzione di codifica e compressione più generale e migliore.

Il numero di caratteri in unicode è fisso e noto, e lo schema unicode-13.0.0 ha 143859 caratteri [(vedi qui](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

È perfettamente possibile contare la frequenza di occorrenza di ogni carattere e poi comprimerlo usando la codifica Hoffman.

Così, usando un corpus cinese, ho iniziato a contare le frequenze delle parole.

Il corpus è il seguente.

* [Corpus cinese di Wikipedia](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel crawler](https://github.com/ma6254/FictionDown) (la versione di rilascio striscia ripetutamente capitoli non validi più e più volte, quindi la versione master è necessaria `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler per la rete BT](https://github.com/gcxfd/bt-spider)
* [Alcuni crawler scritti di getto, vedi la directory dei codici spider](https://github.com/rmw-link/utf8_compress/tree/master/spider)

I risultati sono buoni, tre caratteri cinesi possono essere compressi a 5 byte, che è già oltre la compressione di gb18030.

Mi sono inoltre chiesto se potevo aggiungere parole comuni al dizionario di Hoffman per ottimizzare ulteriormente l'effetto di compressione.

Così ho fatto un dizionario con parole di uso comune (compresso a più di 500 KB) usando [l'algoritmo di formazione nella directory train](https://github.com/rmw-link/rmw-utf8/tree/master/train) per la separazione delle parole + ngram.

L'ho provato e schiaccia ogni algoritmo di compressione sul mercato.

Fico.