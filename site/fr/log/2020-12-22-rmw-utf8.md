# rmw-utf8 - encodage de la compression utf8

Un algorithme de compression de texte court pour utf-8, optimisé pour le chinois, basé sur le langage de programmation rust.

Note : rmw-utf8 peut seulement compresser du texte utf-8, ce n'est pas un algorithme de compression binaire d'usage général.

Il existe une [version rust](https://github.com/rmw-link/rmw-utf8) et une [version wasm](https://github.com/rmw-lib/rmw-utf8-wasm) pour javascript.

## Mode d'emploi

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Évaluation du taux de compression

Cet algorithme est conçu pour compresser des textes courts et les résultats sont les suivants. Comme vous pouvez le constater, plus le texte est court, plus le taux de compression de cet algorithme est élevé.

À 22467 octets (environ 7500 caractères chinois), rmw-utf8 est toujours plus performant que les algorithmes de compression génériques.

```
#include compress_test/test.txt
```

La machine de test est un MacBook Pro 2015 ( 2.2 GHz Intel Core i7 )

Le code de test se trouve à l'adresse suivante : [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Notes sur l'utilisation

La compression remplace à la fois `\r\n` et `\r` dans le texte par `\n`, ce qui signifie que le texte compressé et décompressé peut ne pas être identique.

### Histoire

`\r``\n` La première amène le curseur au début de la ligne et la seconde déplace le curseur vers le bas d'une image.

Il était une fois, bien avant les ordinateurs, une machine appelée Télétype Modèle qui pouvait taper 10 caractères par seconde.

Le problème était qu'il fallait 0,2 seconde pour faire un saut de ligne. Si un nouveau personnage arrivait dans ces 0,2 secondes, le personnage serait perdu.

Les développeurs ont donc eu l'idée d'ajouter deux caractères de fin de ligne à chaque ligne.

L'un est appelé "retour chariot", qui indique à la machine à écrire de positionner la tête d'impression sur le bord gauche, et l'autre est appelé "saut de ligne", qui indique à la machine à écrire de déplacer le papier sur une ligne.

C'est de là que viennent les termes "saut de ligne" et "retour chariot".

Plus tard, lorsque l'ordinateur a été inventé, ces deux concepts ont également été appliqués aux ordinateurs. À l'époque, la mémoire était très chère et certains scientifiques pensaient qu'il était trop coûteux d'ajouter deux caractères à la fin de chaque ligne, donc un seul suffirait.

Alors le monde s'est ouvert.

Sur les systèmes Unix/Linux, le seul caractère à la fin de chaque ligne est un "saut de ligne", `\n`; sur les systèmes Windows, le caractère par défaut est "retour chariot + saut de ligne", `\r\n`; sur les systèmes Mac, le caractère par défaut est "retour chariot". "ou `\r`.

Les éditeurs de texte modernes prennent désormais en charge `\n` comme caractère de fermeture, ce qui rend inutile l'utilisation de `\r`.

## Formation de dictionnaires personnalisés

Il est possible d'entraîner votre propre ensemble de dictionnaires de compression pour différentes langues et différents types de texte afin d'améliorer l'effet de compression.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## La compression du streaming n'est pas encore implémentée

Aucune compression de flux n'a été effectuée (après tout, mon scénario porte principalement sur des textes courts).

Toute personne qui en a besoin peut emballer elle-même une autre compression de flux.

Par exemple, compressez tous les 1 Mo, puis enregistrez le nombre d'octets de contenu compressé au début de chaque paragraphe après la compression.

## Principes de codage

Les textes d'une douzaine à quelques centaines de caractères, principalement en chinois, ne conviennent pas aux algorithmes de compression généraux.

Par exemple, j'ai testé [zstd](https://github.com/facebook/zstd), l'algorithme de compression le plus puissant du monde, et il comprimait souvent 42 octets en 62 octets (oui, il agrandissait au lieu de compresser), même lors de l'apprentissage de dictionnaires (je n'ai pas réussi à faire en sorte que zstd construise des dictionnaires par incréments de 3 octets ; j'ai examiné le dictionnaire de zstd et il était plein de phrases courtes).

Il existe quelques algorithmes de compression axés sur les textes courts, tels que [shoco](https://ed-von-schleck.github.io/shoco/) et [smaz](https://github.com/antirez/smaz), mais ceux-ci ne fonctionnent que pour les langues de type anglais et amplifient encore le chinois court (leurs dictionnaires ne comptent que quelques centaines de caractères, ce qui n'est pas suffisant, de sorte que même le recyclage des dictionnaires n'est pas envisageable).

Une autre option de compression consiste à modifier l'encodage du texte.

Si vous connaissez un tant soit peu le codage unicode, vous comprendrez que le schéma de codage utf-8 nécessite trois octets d'espace de stockage pour un caractère chinois (ce qui est en fait un gaspillage).

En gb18030, un caractère chinois occupe deux octets, ce qui permet d'économiser 33 % de l'espace. Cependant, gb18030 ne couvre pas tous les caractères unicode (il ne s'agit que d'un sous-ensemble de utf8) et ne peut être utilisé.

Il existe des codages de compression unicode standardisés tels que [scsu](https://github.com/dop251/scsu)[(utilisé par SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) et [utf-c](https://github.com/deNULL/utf-c).

Je l'ai [testé](https://denull.github.io/utf-c) et cela représente environ deux octets par chinois, plus un octet supplémentaire (par exemple, 4 chinois représentent environ 2*4+1 = 9 octets).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

L'essentiel est que j'ai cherché sur le web et que je n'ai trouvé aucune implémentation de la rouille pour ces deux codages.

Il n'est pas impossible d'écrire ma propre implémentation rust de ces encodages, mais cela nécessite une compréhension approfondie des intervalles des tables de codes des différents langages unicode, ce qui est coûteux à apprendre.

Je me suis donc demandé si je pouvais créer une solution d'encodage et de compression plus générale et plus efficace.

Le nombre de caractères dans l'unicode est fixe et connu, et le schéma unicode-13.0.0 comporte 143859 caractères [(voir ici](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Il est parfaitement possible de compter la fréquence d'apparition de chaque caractère, puis de le compresser en utilisant le codage Hoffman.

Donc, en utilisant un corpus chinois, j'ai commencé à compter la fréquence des mots.

Le corpus est le suivant.

* [Corpus chinois de Wikipedia](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown, un robot d'exploration de romans en ligne](https://github.com/ma6254/FictionDown) (la version de base va répéter à l'infini l'exploration de chapitres non valides, la version maître est donc nécessaire `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler pour le réseau BT](https://github.com/gcxfd/bt-spider)
* [Quelques crawlers écrits à l'improviste, voir le répertoire des spiders de code](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Les résultats sont bons, trois caractères chinois peuvent être compressés à 5 octets, ce qui est déjà au-delà de la compression de gb18030.

Je me suis également demandé si je pouvais ajouter des mots courants au dictionnaire d'Hoffman pour optimiser encore l'effet de compression.

J'ai donc créé un dictionnaire avec des mots couramment utilisés (compressé à plus de 500 Ko) en utilisant [l'algorithme d'entraînement dans le répertoire train](https://github.com/rmw-link/rmw-utf8/tree/master/train) pour la séparation des mots + ngram.

Je l'ai testé et il écrase tous les algorithmes de compression du marché.

Cool.