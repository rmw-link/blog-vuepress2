# rmw-utf8 - кодирование сжатия utf8

Алгоритм сжатия короткого текста для utf-8, оптимизированный для китайского языка, основанный на языке программирования rust.

Примечание: rmw-utf8 может сжимать только текст в формате utf-8, он не является алгоритмом сжатия двоичных файлов общего назначения.

Существует [версия rust](https://github.com/rmw-link/rmw-utf8) и [версия wasm](https://github.com/rmw-lib/rmw-utf8-wasm) для javascript.

## Как использовать

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Оценка степени сжатия

Этот алгоритм предназначен для сжатия коротких текстов, и его результаты выглядят следующим образом. Как видно, чем короче текст, тем лучше степень сжатия данного алгоритма.

При размере 22467 байт (около 7500 китайских иероглифов) rmw-utf8 по-прежнему превосходит общие алгоритмы сжатия.

```
#include compress_test/test.txt
```

В качестве тестовой машины используется MacBook Pro 2015 (2,2 ГГц Intel Core i7).

Код теста можно найти по адресу [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Примечания по использованию

Сжатие заменяет в тексте `\r\n` и `\r` на `\n`, что означает, что сжатый и распакованный текст могут быть не идентичны.

### История

`\r``\n` Первая переводит курсор в начало строки, а вторая перемещает курсор на один кадр вниз.

Когда-то давно, задолго до появления компьютеров, существовала машина под названием Teletype Model, которая могла набирать 10 символов в секунду.

Проблема заключалась в том, что на перенос строки уходило 0,2 секунды. Если за эти 0,2 секунды появится новый персонаж, он будет потерян.

Поэтому разработчикам пришла в голову идея добавить в каждую строку два символа конца строки.

Один из них называется "возврат каретки", который указывает пишущей машинке установить печатающую головку у левой границы, а другой называется "подача строки", который указывает машинке переместить бумагу на одну строку вниз.

Отсюда происходят понятия "перевод строки" и "возврат каретки".

Позже, когда был изобретен компьютер, эти две концепции были применены и к компьютерам. В то время память была очень дорогой, и некоторые ученые считали, что добавлять два символа в конце каждой строки слишком расточительно, поэтому достаточно одного.

Так мир раскололся.

В системах Unix/Linux единственным символом в конце каждой строки является "перевод строки", `\n`; в системах Windows по умолчанию используется "возврат каретки + перевод строки", `\r\n`; в системах Mac по умолчанию используется "возврат каретки " или `\r`.

Современные текстовые редакторы теперь поддерживают `\n` в качестве закрывающего символа, поэтому нет необходимости в `\r`.

## Обучение пользовательским словарям

Можно обучить собственный набор словарей сжатия для разных языков и типов текста, чтобы усилить эффект сжатия.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Потоковое сжатие еще не реализовано

Потоковое сжатие не производилось (в конце концов, мой сценарий - это в основном короткие тексты).

Тот, кому это нужно, может самостоятельно упаковать еще одну потоковую компрессию.

Например, сжимайте каждый 1 МБ, а затем записывайте количество байт сжатого содержимого в начале каждого абзаца после сжатия.

## Принципы кодирования

Текст размером от десятка до пары сотен символов, в основном на китайском языке, не подходит для алгоритмов сжатия общего назначения.

Например, я тестировал [zstd](https://github.com/facebook/zstd), самый мощный алгоритм сжатия в мире, и он часто сжимал 42 байта в 62 байта (да, он увеличивал вместо сжатия), даже при обучении словарей (я не мог понять, как заставить zstd строить словари с шагом в 3 байта; я просмотрел словарь zstd, и он был полон коротких предложений).

Существуют некоторые алгоритмы сжатия коротких текстов, такие как [shoco](https://ed-von-schleck.github.io/shoco/) и [smaz](https://github.com/antirez/smaz), но они работают только для англоподобных языков и все еще усиливают короткий китайский язык (их словари имеют длину всего несколько сотен символов, что недостаточно, поэтому даже переобучение словарей нецелесообразно).

Другой вариант сжатия - изменение кодировки текста.

Если вы знаете что-нибудь о кодировке unicode, вы поймете, что схема кодировки utf-8 требует три байта пространства для хранения одного китайского символа (что на самом деле довольно расточительно).

В gb18030 один китайский иероглиф занимает два байта, экономя 33% пространства. Однако gb18030 не охватывает все символы Юникода (это только подмножество utf8) и не может быть использован.

Существуют стандартизированные кодировки сжатия юникода, такие как [scsu](https://github.com/dop251/scsu)[(используется в SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) и [utf-c](https://github.com/deNULL/utf-c).

Я [протестировал](https://denull.github.io/utf-c) это, и это примерно два байта на китайский язык, плюс дополнительный байт (например, 4 китайских языка - это примерно 2*4+1 = 9 байт).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Главное, что я искал в Интернете и не нашел ни одной реализации rust для этих двух кодировок.

Написание собственной rust-реализации этих кодировок не является невозможным, но это требует глубокого понимания интервалов кодовых таблиц различных юникодовых языков, изучение которых требует больших затрат.

Поэтому мне стало интересно, смогу ли я создать более общее и лучшее решение для кодирования и сжатия.

Количество символов в юникоде фиксировано и известно, и схема unicode-13.0.0 содержит 143859 символов [(см. здесь](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Вполне возможно подсчитать частоту появления каждого символа, а затем сжать его с помощью кодировки Хоффмана.

Поэтому, используя некоторый китайский корпус, я начал подсчитывать частоту слов.

Корпус выглядит следующим образом.

* [Китайский корпус Википедии](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel crawler](https://github.com/ma6254/FictionDown) (релизная версия будет снова и снова перебирать недействительные главы, поэтому необходима мастер-версия `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler для сети BT](https://github.com/gcxfd/bt-spider)
* [Несколько краулеров, написанных на скорую руку, см. каталог кодов пауков](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Результаты хорошие, три китайских символа могут быть сжаты до 5 байт, что уже превышает сжатие gb18030.

Далее я задался вопросом, можно ли добавить обычные слова в словарь Хоффмана, чтобы еще больше оптимизировать эффект сжатия.

Поэтому я составил словарь с часто используемыми словами (сжатый до более чем 500 КБ), используя [алгоритм обучения в каталоге train](https://github.com/rmw-link/rmw-utf8/tree/master/train) для разделения слов + ngram.

Попробовал его, и он сокрушает все алгоритмы сжатия на рынке.

Круто.