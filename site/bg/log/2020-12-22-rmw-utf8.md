# rmw-utf8 - кодиране на компресия utf8

Алгоритъм за компресиране на кратък текст за utf-8, оптимизиран за китайски език, базиран на езика за програмиране rust.

Забележка: rmw-utf8 може да компресира само utf-8 текст, не е алгоритъм за двоична компресия с общо предназначение.

Съществува [версия rust](https://github.com/rmw-link/rmw-utf8) и [версия wasm](https://github.com/rmw-lib/rmw-utf8-wasm) за javascript.

## Как да използвате

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Оценка на степента на компресия

Този алгоритъм е предназначен за компресиране на кратки текстове и резултатите са следните. Както можете да видите, колкото по-кратък е текстът, толкова по-добра е степента на компресия на този алгоритъм.

При 22467 байта (около 7500 китайски знака) rmw-utf8 все още превъзхожда общите алгоритми за компресиране.

```
#include compress_test/test.txt
```

Тестовата машина е MacBook Pro 2015 ( 2,2 GHz Intel Core i7 )

Тестовият код може да бъде намерен в [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Бележки за употреба

Компресирането заменя `\r\n` и `\r` в текста с `\n`, което означава, че компресираният и декомпресираният текст може да не са идентични.

### История

`\r``\n` Първата опция премества курсора в началото на реда, а втората - с един кадър надолу.

Някога, много преди появата на компютрите, е имало машина, наречена Teletype Model, която е можела да набира 10 знака в секунда.

Проблемът беше, че прекъсването на реда отнемаше 0,2 секунди. Ако през тези 0,2 секунди се появи нов персонаж, той ще бъде изгубен.

Така че разработчиците стигнаха до идеята да добавят два символа за край на реда към всеки ред.

Едната се нарича "връщане на каретата", която указва на пишещата машина да позиционира печатащата глава на лявата граница, а другата се нарича "подаване на реда", която указва на пишещата машина да придвижи хартията с един ред надолу.

Оттук идват понятията "line feed" и "carriage return".

По-късно, когато е изобретен компютърът, тези две понятия са приложени и към компютрите. По онова време паметта е била много скъпа и някои учени са смятали, че е твърде разточително да се добавят два знака в края на всеки ред, така че е достатъчен един.

Така светът се отвори.

В системите Unix/Linux единственият символ в края на всеки ред е "line feed", `\n`; в системите Windows по подразбиране е "carriage return + line feed", `\r\n`; в системите Mac по подразбиране е "carriage return " или `\r`.

Съвременните текстови редактори вече поддържат `\n` като затварящ символ, така че няма нужда от `\r`.

## Обучение на потребителски речници

Възможно е да обучите собствен набор от речници за компресиране за различни езици и видове текст, за да подобрите ефекта на компресиране.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Все още не е въведено компресиране на стрийминг

Не е извършено компресиране на стрийминг (в крайна сметка моят сценарий включва предимно кратки текстове).

Всеки, който се нуждае от нея, може сам да пакетира друга компресия за стрийминг.

Например компресирайте всеки 1 MB и след това запишете броя на байтовете компресирано съдържание в началото на всеки параграф след компресирането.

## Принципи на кодиране

Текстовете с дължина от десетина до няколкостотин знака, предимно на китайски език, не са подходящи за алгоритми за компресиране с общо предназначение.

Например, тествах [zstd](https://github.com/facebook/zstd), най-мощния алгоритъм за компресиране в света, и той често компресираше 42 байта в 62 байта (да, увеличаваше, вместо да компресира), дори когато обучаваше речници (не можах да разбера как да накарам zstd да изгражда речници на стъпки от по 3 байта; прегледах речника на zstd и той беше пълен с кратки изречения).

Съществуват някои алгоритми за компресиране на кратки текстове, като [shoco](https://ed-von-schleck.github.io/shoco/) и [smaz](https://github.com/antirez/smaz), но те работят само за езици, подобни на английския, и все още усилват краткия китайски език (техните речници са дълги само няколкостотин знака, което не е достатъчно, така че дори преквалификацията на речниците е неприложима).

Друга възможност за компресиране е да се промени кодирането на текста.

Ако знаете нещо за кодирането на Unicode, ще разберете, че схемата за кодиране utf-8 изисква три байта пространство за съхранение на един китайски символ (което всъщност е доста разточително).

В gb18030 един китайски йероглиф заема два байта, което спестява 33% от пространството. Въпреки това gb18030 не покрива всички символи на Unicode (той е само подмножество на utf8) и не може да се използва.

Съществуват стандартизирани кодировки за компресиране на Unicode, като [scsu](https://github.com/dop251/scsu)[(използва се от SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) и [utf-c](https://github.com/deNULL/utf-c).

[Тествах](https://denull.github.io/utf-c) това и то е около два байта на китайски език плюс един допълнителен байт (например 4 китайски езика са около 2*4+1 = 9 байта).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Основното е, че претърсих мрежата и не намерих реализации на rust за тези две кодировки.

Написването на собствена реализация на тези кодировки в езика rust не е невъзможно, но изисква задълбочено разбиране на интервалите на кодовата таблица на различните езици с уникод, което е скъпо за научаване.

Затова се зачудих дали мога да направя по-общо и по-добро решение за кодиране и компресиране.

Броят на символите в Unicode е фиксиран и известен, а схемата Unicode-13.0.0 има 143859 символа [(вж. тук](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py) ).

Напълно възможно е да се пресметне честотата на срещане на всеки символ и след това да се компресира с помощта на кодирането на Хофман.

Така че, използвайки китайски корпус, започнах да пресмятам честотата на думите.

Корпусът е следният.

* [Китайски корпус на Wikipedia](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown web novel crawler](https://github.com/ma6254/FictionDown) (версията за пускане на пазара многократно ще претърсва невалидни глави отново и отново, така че е необходима основната версия `go get github.com/ma6254/FictionDown@master`)
* [Weibo crawler](https://github.com/gcxfd/weibo-crawler)
* [DHT crawler за мрежата на BT](https://github.com/gcxfd/bt-spider)
* [Няколко обхождащи програми, написани от ръкава, вижте директорията за паяци с код](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Резултатите са добри, три китайски йероглифа могат да бъдат компресирани до 5 байта, което вече е извън компресията на gb18030.

Освен това се чудех дали мога да добавя общи думи към речника на Хофман, за да оптимизирам допълнително ефекта на компресия.

Затова направих речник с често използвани думи (компресиран до над 500 KB), като използвах [алгоритъма за обучение в директорията train](https://github.com/rmw-link/rmw-utf8/tree/master/train) за разделяне на думи + ngram.

Изпробвах го и той смазва всеки алгоритъм за компресия на пазара.

Готино.