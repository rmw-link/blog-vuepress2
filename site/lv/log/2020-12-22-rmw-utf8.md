# rmw-utf8 - utf8 saspiešanas kodēšana

Īss teksta saspiešanas algoritms utf-8 formātam, optimizēts ķīniešu valodai un balstīts uz rust programmēšanas valodu.

Piezīme: rmw-utf8 var saspiest tikai utf-8 tekstu, tas nav vispārējas nozīmes bināro saspiešanas algoritms.

Ir pieejama [rūsas versija](https://github.com/rmw-link/rmw-utf8) un [wasm versija](https://github.com/rmw-lib/rmw-utf8-wasm) javascript.

## Kā lietot

```rust
use rmw_utf8::{decode, encode};

fn main() {
  let txt = "测试一下";
  let compressed = encode(&txt.as_bytes());
  let decompressed = decode(&compressed[..]);
  assert!(txt == decompressed);
}
```

## Kompresijas ātruma novērtēšana

Šis algoritms ir paredzēts īsu tekstu saspiešanai, un rezultāti ir šādi. Kā redzams, jo īsāks ir teksts, jo labāks ir šā algoritma saspiešanas koeficients.

Ar 22467 baitiem (aptuveni 7500 ķīniešu rakstzīmju) rmw-utf8 joprojām pārspēj vispārējos saspiešanas algoritmus.

```
#include compress_test/test.txt
```

Testa mašīna ir MacBook Pro 2015 ( 2,2 GHz Intel Core i7 ).

Testa kods ir atrodams vietnē [compress_test](https://github.com/rmw-link/rmw-utf8/tree/master/compress_test)

## Piezīmes par lietošanu

Kompresija aizstāj gan `\r\n`, gan `\r` tekstā ar `\n`, kas nozīmē, ka saspiestais un izspiestais teksts var nebūt identisks.

### Vēsture

`\r``\n` Pirmā no tām novirza kursoru uz rindas sākumu, bet otrā pārvieto kursoru par vienu kadru uz leju.

Kādreiz, ilgi pirms datoru rašanās, bija mašīna, ko sauca par Teletype Model, kas spēja drukāt 10 rakstzīmes sekundē.

Problēma bija tāda, ka bija nepieciešamas 0,2 sekundes, lai izveidotu rindas pārrāvumu. Ja šo 0,2 sekunžu laikā ienāktu jauns raksturs, raksturs tiktu zaudēts.

Tāpēc izstrādātāji nāca klajā ar ideju katrai rindai pievienot divus rindas beigu simbolus.

Vienu no tiem sauc par "carriage return", kas liek rakstāmmašīnai novietot drukas galviņu pie kreisās malas, bet otru sauc par "line feed", kas liek rakstāmmašīnai pārvietot papīru par vienu rindu uz leju.

Tieši no šejienes padeves (line feed) un pārvadāšanas atgriešanās (carriage return).

Vēlāk, kad tika izgudrots dators, abi šie jēdzieni tika attiecināti arī uz datoriem. Tajā laikā atmiņa bija ļoti dārga, un daži zinātnieki uzskatīja, ka ir pārāk izšķērdīgi katras rindas beigās pievienot divas rakstzīmes, tāpēc pietika ar vienu.

Pasaule atvērās.

Unix/Linux sistēmās vienīgais raksturs katras rindas beigās ir "line feed", `\n`; Windows sistēmās noklusējuma raksturs ir "carriage return + line feed", `\r\n`; Mac sistēmās noklusējuma raksturs ir "carriage return". " vai `\r`.

Mūsdienu teksta redaktori tagad atbalsta `\n` kā aizvēršanas rakstzīmi, tāpēc `\r` nav nepieciešams.

## Pielāgotu vārdnīcu apmācība

Lai uzlabotu saspiešanas efektu, ir iespējams apmācīt savu saspiešanas vārdnīcu kopumu dažādām valodām un teksta veidiem.

```bash
git clone --depth=1 https://github.com/rmw-link/rmw-utf8.git
cd rmw-utf8
# 在txt目录下放你的准备训练语料，格式为utf8编码的txt文件
cd train
./train.sh
```

## Straumēšanas saspiešana vēl nav ieviesta

Straumēšanas saspiešana nav veikta (galu galā mans scenārijs galvenokārt ir īsi teksti).

Ikviens, kam tas ir nepieciešams, var pats iepakot citu straumēšanas saspiešanu.

Piemēram, saspiežiet katru 1 MB un pēc saspiešanas katras rindkopas sākumā ierakstiet saspiestā satura baitu skaitu.

## Kodēšanas principi

Teksts, kas sastāv no dučiem līdz pāris simtiem rakstzīmju, galvenokārt ķīniešu valodā, nav piemērots vispārējas nozīmes saspiešanas algoritmiem.

Piemēram, es testēju [zstd](https://github.com/facebook/zstd), pasaulē jaudīgāko saspiešanas algoritmu, un tas bieži saspieda 42 baitus 62 baitos (jā, tas palielināja, nevis saspieda), pat tad, kad apmācīja vārdnīcas (es nevarēju izdomāt, kā likt zstd veidot vārdnīcas ar 3 baitu soli; es apskatīju zstd vārdnīcu, un tā bija pilna ar īsiem teikumiem).

Ir daži uz īsiem tekstiem orientēti saspiešanas algoritmi, piemēram, [shoco](https://ed-von-schleck.github.io/shoco/) un [smaz](https://github.com/antirez/smaz), taču tie darbojas tikai angļu valodām līdzīgās valodās un joprojām pastiprina īso ķīniešu valodu (to vārdnīcas ir tikai dažus simtus rakstzīmju garas, kas nav pietiekami, tāpēc pat vārdnīcu pārkvalificēšana nav iespējama).

Vēl viena saspiešanas iespēja ir mainīt teksta kodējumu.

Ja kaut ko zināt par unikoda kodēšanu, jūs sapratīsiet, ka utf-8 kodēšanas shēmā vienam ķīniešu rakstzīmēm ir nepieciešami trīs baiti glabāšanas vietas (kas patiesībā ir diezgan nelietderīgi).

Programmā gb18030 viena ķīniešu rakstzīme aizņem divus baitus, ietaupot 33% vietas. Tomēr gb18030 neaptver visas unikoda rakstzīmes (tā ir tikai utf8 apakškopa), un to nevar izmantot.

Pastāv standartizētas vienkodes saspiešanas kodes, piemēram, [scsu](https://github.com/dop251/scsu) ( [ko izmanto SqlServer](https://docs.microsoft.com/en-us/sql/relational-databases/data-compression/unicode-compression-implementation?view=sql-server-ver15) ) un [utf-c](https://github.com/deNULL/utf-c).

Esmu to [pārbaudījis,](https://denull.github.io/utf-c) un tas ir aptuveni divi baiti uz ķīniešu valodu, plus papildu baits (piemēram, 4 ķīniešu valodas ir aptuveni 2*4+1 = 9 baiti).

![](https://raw.githubusercontent.com/gcxfd/img/gh-pages/ffxMd3.jpg)

Galvenais ir tas, ka esmu meklējis tīmeklī un neatradu nevienu šo divu kodējumu implementāciju.

Uzrakstīt savu rust implementāciju šīm kodēm nav neiespējami, taču tas prasa dziļu izpratni par dažādu vienkodes valodu kodu tabulu intervāliem, kas ir dārgi apgūstams.

Tāpēc es domāju, vai es varētu izveidot vispārīgāku un labāku kodēšanas un saspiešanas risinājumu.

Unikoda rakstzīmju skaits ir fiksēts un zināms, un unikoda-13.0.0 shēmā ir 143859 rakstzīmes [(skatīt šeit)](https://github.com/rmw-link/utf8_compress/blob/master/all_char.py).

Ir pilnīgi iespējams saskaitīt katras rakstzīmes sastopamības biežumu un pēc tam to saspiest, izmantojot Hoffmana kodējumu.

Tāpēc, izmantojot kādu ķīniešu valodas korpusu, es sāku skaitīt vārdu biežumu.

Korpuss ir šāds.

* [Vikipēdijas ķīniešu valodas korpuss](https://jdhao.github.io/2019/01/10/two_chinese_corpus)
* [FictionDown tīmekļa romānu pārlūkošanas programma](https://github.com/ma6254/FictionDown) (versijas versijā atkārtoti tiks pārlūkoti nederīgas nodaļas, tāpēc ir nepieciešama galvenā versija `go get github.com/ma6254/FictionDown@master`)
* [Weibo rāpotājs](https://github.com/gcxfd/weibo-crawler)
* [BT tīkla DHT pārlūks](https://github.com/gcxfd/bt-spider)
* [Daži rāpotāji rakstīts pie aproces, skatiet kodu zirnekļa direktoriju](https://github.com/rmw-link/utf8_compress/tree/master/spider)

Rezultāti ir labi, trīs ķīniešu rakstzīmes var saspiest līdz 5 baitiem, kas jau pārsniedz gb18030 saspiešanas iespējas.

Vēl es domāju, vai varētu Hoffman vārdnīcai pievienot kopīgus vārdus, lai vēl vairāk optimizētu saspiešanas efektu.

Tāpēc es izveidoju vārdnīcu ar biežāk lietotiem vārdiem (saspiesta līdz vairāk nekā 500 KB), izmantojot [apmācības algoritmu, kas atrodas katalogā train](https://github.com/rmw-link/rmw-utf8/tree/master/train), lai atdalītu vārdus + ngram.

Izmēģināju to, un tas pārspēj visus tirgū pieejamos kompresijas algoritmus.

Forši.